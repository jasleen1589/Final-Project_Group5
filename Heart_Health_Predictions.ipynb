{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNr19bKVeT69zXdHrl3Fo0z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasleen1589/Final-Project_Group5/blob/jkaur/Heart_Health_Predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_HI7-jx_tCm",
        "outputId": "89254616-91cd-420a-b985-4bb668f10427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Dependecies\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you've uploaded the CSV to your Google Colab session\n",
        "heart_df = pd.read_csv('heart_disease_health_indicators.csv')\n"
      ],
      "metadata": {
        "id": "OHT_MlXRg5AW"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the data"
      ],
      "metadata": {
        "id": "-7adO_ZpB5Wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Identify missing values\n",
        "heart_df.info()"
      ],
      "metadata": {
        "id": "6saeCXe-FL5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b0f5b33-3d48-4bd6-9ded-a99c04590712"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 253661 entries, 0 to 253660\n",
            "Data columns (total 22 columns):\n",
            " #   Column                Non-Null Count   Dtype\n",
            "---  ------                --------------   -----\n",
            " 0   HeartDiseaseorAttack  253661 non-null  int64\n",
            " 1   HighBP                253661 non-null  int64\n",
            " 2   HighChol              253661 non-null  int64\n",
            " 3   CholCheck             253661 non-null  int64\n",
            " 4   BMI                   253661 non-null  int64\n",
            " 5   Smoker                253661 non-null  int64\n",
            " 6   Stroke                253661 non-null  int64\n",
            " 7   Diabetes              253661 non-null  int64\n",
            " 8   PhysActivity          253661 non-null  int64\n",
            " 9   Fruits                253661 non-null  int64\n",
            " 10  Veggies               253661 non-null  int64\n",
            " 11  HvyAlcoholConsump     253661 non-null  int64\n",
            " 12  AnyHealthcare         253661 non-null  int64\n",
            " 13  NoDocbcCost           253661 non-null  int64\n",
            " 14  GenHlth               253661 non-null  int64\n",
            " 15  MentHlth              253661 non-null  int64\n",
            " 16  PhysHlth              253661 non-null  int64\n",
            " 17  DiffWalk              253661 non-null  int64\n",
            " 18  Sex                   253661 non-null  int64\n",
            " 19  Age                   253661 non-null  int64\n",
            " 20  Education             253661 non-null  int64\n",
            " 21  Income                253661 non-null  int64\n",
            "dtypes: int64(22)\n",
            "memory usage: 42.6 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing the null values\n",
        "\n",
        "heart_df.isnull().sum()"
      ],
      "metadata": {
        "id": "jGYuFaitFhmB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40c6070a-6af9-4464-e9d3-41ed67d80a70"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HeartDiseaseorAttack    0\n",
              "HighBP                  0\n",
              "HighChol                0\n",
              "CholCheck               0\n",
              "BMI                     0\n",
              "Smoker                  0\n",
              "Stroke                  0\n",
              "Diabetes                0\n",
              "PhysActivity            0\n",
              "Fruits                  0\n",
              "Veggies                 0\n",
              "HvyAlcoholConsump       0\n",
              "AnyHealthcare           0\n",
              "NoDocbcCost             0\n",
              "GenHlth                 0\n",
              "MentHlth                0\n",
              "PhysHlth                0\n",
              "DiffWalk                0\n",
              "Sex                     0\n",
              "Age                     0\n",
              "Education               0\n",
              "Income                  0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns_with_missing_values = ['Diabetes', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income']\n",
        "df_cleaned_specific = df.dropna(subset=columns_with_missing_values)\n"
      ],
      "metadata": {
        "id": "NgQztk4yGVf3"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Pre-Processing"
      ],
      "metadata": {
        "id": "KLBJxKfhB7RG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming there are no categorical variables to encode in this dataset\n",
        "# Select features and target\n",
        "X = df.drop('HeartDiseaseorAttack', axis=1)  # Features\n",
        "y = df['HeartDiseaseorAttack']  # Target variable\n"
      ],
      "metadata": {
        "id": "4aNjlCp8B2HI"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the Data"
      ],
      "metadata": {
        "id": "2CsJvxp8IX9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "eMbgy6UDCC5Z"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Scaling"
      ],
      "metadata": {
        "id": "NgWW9W5cCGXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "NihtpqrGCIpX"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the Model"
      ],
      "metadata": {
        "id": "UTgRAywDCLS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn"
      ],
      "metadata": {
        "id": "xMRmydh-HrDm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceaf3ec3-8b47-487b-c353-bd203d17d809"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "XTrqnm9BHuH_"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(random_state=42)"
      ],
      "metadata": {
        "id": "Zj4TzGfWHgxU"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Example of initializing a RandomForestClassifier, which does use n_estimators\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "YwYU-regII27"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train.head())"
      ],
      "metadata": {
        "id": "pHrovP61JHal",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a54ee6e-28d0-48e3-fe87-b1995eca9857"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97959     0\n",
            "116345    0\n",
            "128909    0\n",
            "31522     0\n",
            "82000     0\n",
            "Name: HeartDiseaseorAttack, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train.isna().sum())"
      ],
      "metadata": {
        "id": "j0KjPyHHJJwl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9c6bbfc-f139-4182-9786-e4c2f5251158"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If y_train contains missing values:\n",
        "y_train = y_train.dropna()"
      ],
      "metadata": {
        "id": "IrZHJ6gvJMkQ"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n",
        "from sklearn.impute import SimpleImputer"
      ],
      "metadata": {
        "id": "ZE0xXlfyJRWO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c739a80-8348-457a-e901-caf42a5cb14d"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imputer = SimpleImputer(strategy=\"mean\")\n",
        "y_train = imputer.fit_transform(y_train.values.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "hm1fa-5QKDQS"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "\n",
        "# Create an imputer object with a strategy of filling missing values with the mean of the column\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "\n",
        "# Fit on the training data and transform it\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "\n",
        "# Don't forget to transform the test data as well, using the same imputer (without fitting it again)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "# Now you can fit the model with the imputed data\n",
        "model.fit(X_train_imputed, y_train)\n"
      ],
      "metadata": {
        "id": "g71U_VrtLGFE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "e34b8c34-e39e-4999-a894-20c2b344c1c9"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-137-33d905ddc70c>:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  model.fit(X_train_imputed, y_train)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with any missing values\n",
        "df_cleaned = df.dropna()\n",
        "\n",
        "# After cleaning, proceed with your feature/target selection, data splitting, and so on\n"
      ],
      "metadata": {
        "id": "u2oYCu5ELR40"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "# Initialize the model that can handle NaN values\n",
        "model_with_nan_support = HistGradientBoostingClassifier()\n",
        "\n",
        "# This model can be directly fitted with data containing NaNs\n",
        "model_with_nan_support.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "9J8PaodiLWaH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "b31a5798-2fb6-443b-9164-792a2ccb5665"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HistGradientBoostingClassifier()"
            ],
            "text/html": [
              "<style>#sk-container-id-11 {color: black;background-color: white;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HistGradientBoostingClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HistGradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>HistGradientBoostingClassifier()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model.fit?"
      ],
      "metadata": {
        "id": "Am6_cvwaLdSt"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Xq4ajtMXLhka",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "b8debc81-518e-4b8b-e138-5b690ee3050e"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-140-d768f88d541e>:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  model.fit(X_train, y_train)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-12 {color: black;background-color: white;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After fitting the model, you can use it to make predictions or assess its performance\n",
        "# Correct way to make predictions\n",
        "#y_pred = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "V1GMFByXOG2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the Model"
      ],
      "metadata": {
        "id": "5InjNMw1IbLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values\n",
        "    ('scaler', StandardScaler()),  # Scale features\n",
        "    ('classifier', LogisticRegression(random_state=42))  # Classifier\n",
        "])\n",
        "\n",
        "# Define a grid of parameters to search\n",
        "param_grid = {\n",
        "    'imputer__strategy': ['mean', 'median'],  # Try different strategies for imputation\n",
        "    'classifier__C': [0.1, 1, 10],  # Regularization strength for LogisticRegression\n",
        "    'classifier__solver': ['liblinear', 'saga']  # Optimization algorithms\n",
        "}"
      ],
      "metadata": {
        "id": "Xp3ZXj0Eh0sY"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data using the pipeline\n",
        "y_pred = pipeline.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN7ezxkBUOGn",
        "outputId": "50bd6ee6-e39b-432a-e357-83eed82a3bf0"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "qDsQ9MaJOoPf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "outputId": "745f0c40-de78-4d3e-e9fa-2fcdd4949f94"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9072280854459073\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.99      0.95     24985\n",
            "           1       0.53      0.12      0.20      2588\n",
            "\n",
            "    accuracy                           0.91     27573\n",
            "   macro avg       0.72      0.55      0.57     27573\n",
            "weighted avg       0.88      0.91      0.88     27573\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Optimize the Model**\n",
        "\n"
      ],
      "metadata": {
        "id": "mJccKTUOasxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a grid of parameters to search\n",
        "param_grid = {\n",
        "    'imputer__strategy': ['mean', 'median'],  # Try different strategies for imputation\n",
        "    'classifier__C': [0.1, 1, 10],  # Regularization strength for LogisticRegression\n",
        "    'classifier__solver': ['liblinear', 'saga']  # Optimization algorithms\n",
        "}"
      ],
      "metadata": {
        "id": "4X-AeQBKamqF"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "XjyXSi5acWYX"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the pipeline and parameter grid\n",
        "pipeline = ...\n",
        "param_grid = ..."
      ],
      "metadata": {
        "id": "WTy9gOhCcZ0Y"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the GridSearchCV object\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1)"
      ],
      "metadata": {
        "id": "vakHlqvqcc0V"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "_V2G-i2qegEM"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.DataFrame(X_train)\n",
        "y_train = pd.DataFrame(y_train)"
      ],
      "metadata": {
        "id": "OfKxnBHbUfS2"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.head())\n",
        "print(y_train.head())"
      ],
      "metadata": {
        "id": "L7VTiW3_elAD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50333dad-4eee-46a3-d464-e65b40f0b3d7"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        0         1         2         3         4         5         6   \\\n",
            "0  1.15323 -0.861527  0.196519  0.223245 -0.896897 -0.207032 -0.425294   \n",
            "1  1.15323 -0.861527  0.196519  0.798741 -0.896897 -0.207032 -0.425294   \n",
            "2 -0.86713 -0.861527  0.196519 -0.783872 -0.896897 -0.207032 -0.425294   \n",
            "3 -0.86713 -0.861527  0.196519 -1.503241 -0.896897 -0.207032 -0.425294   \n",
            "4 -0.86713 -0.861527 -5.088560  0.510993  1.114955 -0.207032 -0.425294   \n",
            "\n",
            "         7         8         9   ...        11        12        13        14  \\\n",
            "0 -1.780336  0.753199  0.484249  ...  0.225525 -0.295715  0.466729 -0.425506   \n",
            "1 -1.780336  0.753199  0.484249  ...  0.225525 -0.295715  0.466729 -0.425506   \n",
            "2 -1.780336  0.753199 -2.065052  ...  0.225525 -0.295715  1.404969 -0.425506   \n",
            "3  0.561692  0.753199  0.484249  ...  0.225525 -0.295715  0.466729 -0.013376   \n",
            "4  0.561692  0.753199  0.484249  ...  0.225525 -0.295715 -0.471510 -0.150753   \n",
            "\n",
            "         15        16        17        18       19        20  \n",
            "0 -0.483517 -0.445100 -0.884733  0.631794 -1.09985  0.928495  \n",
            "1  2.985784 -0.445100  1.130284 -0.351821 -0.07421  0.928495  \n",
            "2  1.829351  2.246685 -0.884733  0.959666 -1.09985  0.440835  \n",
            "3 -0.483517  2.246685 -0.884733 -0.023949  0.95143  0.440835  \n",
            "4 -0.483517 -0.445100 -0.884733 -1.663307 -0.07421  0.928495  \n",
            "\n",
            "[5 rows x 21 columns]\n",
            "     0\n",
            "0  0.0\n",
            "1  0.0\n",
            "2  0.0\n",
            "3  0.0\n",
            "4  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tbtCaYpfeSzk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa2e4007-10e1-44c8-9a44-3171490cbd58"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "EUnhf2QhVlFe"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7A8kfcsX1ph",
        "outputId": "dc27c616-092f-4f11-dfda-debb39ca7fb3"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             0         1         2         3         4         5         6   \\\n",
            "0       1.15323 -0.861527  0.196519  0.223245 -0.896897 -0.207032 -0.425294   \n",
            "1       1.15323 -0.861527  0.196519  0.798741 -0.896897 -0.207032 -0.425294   \n",
            "2      -0.86713 -0.861527  0.196519 -0.783872 -0.896897 -0.207032 -0.425294   \n",
            "3      -0.86713 -0.861527  0.196519 -1.503241 -0.896897 -0.207032 -0.425294   \n",
            "4      -0.86713 -0.861527 -5.088560  0.510993  1.114955 -0.207032 -0.425294   \n",
            "...         ...       ...       ...       ...       ...       ...       ...   \n",
            "110285  1.15323  1.160730  0.196519  0.223245 -0.896897  4.830162  2.442677   \n",
            "110286  1.15323 -0.861527  0.196519  2.381353  1.114955 -0.207032 -0.425294   \n",
            "110287  1.15323  1.160730  0.196519  0.079371  1.114955 -0.207032 -0.425294   \n",
            "110288 -0.86713  1.160730  0.196519 -0.496124 -0.896897 -0.207032 -0.425294   \n",
            "110289  1.15323 -0.861527  0.196519  0.942614 -0.896897 -0.207032  2.442677   \n",
            "\n",
            "              7         8         9   ...        11        12        13  \\\n",
            "0      -1.780336  0.753199  0.484249  ...  0.225525 -0.295715  0.466729   \n",
            "1      -1.780336  0.753199  0.484249  ...  0.225525 -0.295715  0.466729   \n",
            "2      -1.780336  0.753199 -2.065052  ...  0.225525 -0.295715  1.404969   \n",
            "3       0.561692  0.753199  0.484249  ...  0.225525 -0.295715  0.466729   \n",
            "4       0.561692  0.753199  0.484249  ...  0.225525 -0.295715 -0.471510   \n",
            "...          ...       ...       ...  ...       ...       ...       ...   \n",
            "110285  0.561692  0.753199  0.484249  ...  0.225525 -0.295715  1.404969   \n",
            "110286  0.561692  0.753199 -2.065052  ...  0.225525  3.381634 -1.409750   \n",
            "110287  0.561692 -1.327670  0.484249  ...  0.225525 -0.295715  0.466729   \n",
            "110288  0.561692  0.753199  0.484249  ...  0.225525 -0.295715 -0.471510   \n",
            "110289 -1.780336  0.753199  0.484249  ...  0.225525 -0.295715  0.466729   \n",
            "\n",
            "              14        15        16        17        18       19        20  \n",
            "0      -0.425506 -0.483517 -0.445100 -0.884733  0.631794 -1.09985  0.928495  \n",
            "1      -0.425506  2.985784 -0.445100  1.130284 -0.351821 -0.07421  0.928495  \n",
            "2      -0.425506  1.829351  2.246685 -0.884733  0.959666 -1.09985  0.440835  \n",
            "3      -0.013376 -0.483517  2.246685 -0.884733 -0.023949  0.95143  0.440835  \n",
            "4      -0.150753 -0.483517 -0.445100 -0.884733 -1.663307 -0.07421  0.928495  \n",
            "...          ...       ...       ...       ...       ...      ...       ...  \n",
            "110285 -0.425506 -0.252230  2.246685 -0.884733  1.287537 -0.07421 -0.534485  \n",
            "110286  1.635143 -0.483517 -0.445100 -0.884733 -1.007564 -1.09985 -2.485125  \n",
            "110287 -0.425506 -0.483517 -0.445100  1.130284  0.959666  0.95143  0.440835  \n",
            "110288 -0.425506 -0.136587 -0.445100 -0.884733  0.303923  0.95143  0.928495  \n",
            "110289  1.635143 -0.483517  2.246685 -0.884733  0.959666 -0.07421 -0.046825  \n",
            "\n",
            "[110290 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(df.columns.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TUyIBayYXm9",
        "outputId": "a677972f-9c6d-4c77-83da-6ba6697b3129"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(110290, 21)\n",
            "(22,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols_transformed = X_train.shape[1]\n",
        "num_cols_original = df.columns.shape[0]\n",
        "\n",
        "print(f\"Number of columns in transformed data: {num_cols_transformed}\")\n",
        "print(f\"Number of columns in original DataFrame: {num_cols_original}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xvDHb6cYa5o",
        "outputId": "70d481f1-4bc1-45ea-fe5b-28e3a12eba99"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of columns in transformed data: 21\n",
            "Number of columns in original DataFrame: 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)"
      ],
      "metadata": {
        "id": "vb7vCtNNYjpg"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the GridSearchCV object\n",
        "#grid_search.fit(X_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ya49RPJCchEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(locals())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbIIECeRY8Li",
        "outputId": "b0c8e427-07d0-4eec-e9f6-9056bbbedc0f"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', \"# Define a grid of parameters to search\\nparam_grid = {\\n    'imputer__strategy': ['mean', 'median'],  # Try different strategies for imputation\\n    'classifier__C': [0.1, 1, 10],  # Regularization strength for LogisticRegression\\n    'classifier__solver': ['liblinear', 'saga']  # Optimization algorithms\\n}\", '# Set up GridSearchCV\\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\\'accuracy\\', verbose=1)\\n\\n# Fit GridSearchCV\\ngrid_search.fit(X_train_scaled, y_train)\\n\\n# Best parameters found\\nprint(\"Best parameters:\", grid_search.best_params_)\\n\\n# Evaluate on the test set\\ny_pred = grid_search.predict(X_test_scaled)\\nprint(classification_report(y_test, y_pred))', '# Set up GridSearchCV\\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\\'accuracy\\', verbose=1)\\ngrid_search.df\\n\\n# Fit GridSearchCV\\ngrid_search.fit(X_train_scaled, y_train)\\n\\n# Best parameters found\\nprint(\"Best parameters:\", grid_search.best_params_)\\n\\n# Evaluate on the test set\\ny_pred = grid_search.predict(X_test_scaled)\\nprint(classification_report(y_test, y_pred))', 'from sklearn.model_selection import GridSearchCV', '# Define the pipeline and parameter grid\\npipeline = ...\\nparam_grid = ...', \"# Instantiate the GridSearchCV object\\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1)\", '# Fit the GridSearchCV object\\ngrid_search.fit(X_train_scaled, y_train)', '# Fit the GridSearchCV object\\ngrid_search.fit(X_train, y_train)', '# Fit the GridSearchCV object\\ngrid_search.fit(X_train_scaled, y_train)\\nX-train_scaled.df', '# Fit the GridSearchCV object\\ngrid_search.fit(X_train_scaled, y_train)', '# Load the data into X_train_scaled and y_train variables\\n# Replace the following lines with your actual data loading code\\nX_train_scaled = pd.read_csv(\"X_train_scaled.csv\")\\ny_train = pd.read_csv(\"y_train.csv\")', 'import pandas as pd', '# Load the data into X_train_scaled and y_train variables\\n# Replace the following lines with your actual data loading code\\nX_train_scaled = pd.read_csv(\"X_train_scaled.csv\")\\ny_train = pd.read_csv(\"y_train.csv\")', '# Load the data into X_train_scaled and y_train variables\\n# Replace the following lines with your actual data loading code\\nX_train_scaled = pd.read_csv(\"heart_disease_health_indicators.csv\")\\ny_train = pd.read_csv(\"heart_disease_health_indicators.csv\")', '# Load the data into X_train_scaled and y_train variables\\n# Replace the following lines with your actual data loading code\\nX_train_scaled = pd.read_csv(\"heart_disease_health_indicators.csv\")\\ny_train = pd.read_csv(\"heart_disease_health_indicators.csv\")', 'print(X_train_scaled.head())\\nprint(y_train.head())', '# Load the data into X_train_scaled and y_train variables\\n# Replace the following lines with your actual data loading code\\nX_train = pd.read_csv(\"heart_disease_health_indicators.csv\")\\ny_train = pd.read_csv(\"heart_disease_health_indicators.csv\")', '# Load the data into X_train_scaled and y_train variables\\n# Replace the following lines with your actual data loading code\\nX_train = pd.read_csv(\"/Resources/heart_disease_health_indicators.csv\")\\ny_train = pd.read_csv(\"Resources/heart_disease_health_indicators.csv\")', 'print(X_train.head())\\nprint(y_train.head())', 'print(X_train.head())\\nprint(y_train.head())', \"import numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\", \"# Assuming you've uploaded the CSV to your Google Colab session\\nheart_df = pd.read_csv('heart_disease_health_indicators.csv')\", '#Identify missing values\\nmissing_values = heart_df.info()', \"# Assuming you've uploaded the CSV to your Google Colab session\\ndf = pd.read_csv('/content/heart_disease_health_indicators.csv')\", \"# Assuming you've uploaded the CSV to your Google Colab session\\ndf = pd.read_csv('/content/heart_disease_health_indicators.csv')\", '#Identify missing values\\nmissing_values = heart_df.info()', '#Removing the null values\\n\\ndf.isnull().sum()', \"# Assuming you've uploaded the CSV to your Google Colab session\\nheart_df = pd.read_csv('/content/heart_disease_health_indicators.csv')\", '#Identify missing values\\nmissing_values = heart_df.info()', '#Removing the null values\\n\\ndf.isnull().sum()', \"columns_with_missing_values = ['Diabetes', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income']\\ndf_cleaned_specific = df.dropna(subset=columns_with_missing_values)\", \"import numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\", \"# Assuming you've uploaded the CSV to your Google Colab session\\nheart_df = pd.read_csv('/content/heart_disease_health_indicators.csv')\", '#Identify missing values\\nmissing_values = heart_df.info()', '#Removing the null values\\n\\ndf.isnull().sum()', \"columns_with_missing_values = ['Diabetes', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income']\\ndf_cleaned_specific = df.dropna(subset=columns_with_missing_values)\", '#df.isnull().sum()', \"# Assuming there are no categorical variables to encode in this dataset\\n# Select features and target\\nX = df.drop('HeartDiseaseorAttack', axis=1)  # Features\\ny = df['HeartDiseaseorAttack']  # Target variable\", 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)', 'scaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)', \"get_ipython().run_line_magic('pip', 'install scikit-learn')\", 'from sklearn.linear_model import LogisticRegression', 'model = LogisticRegression(random_state=42)', 'from sklearn.ensemble import RandomForestClassifier\\n\\n# Example of initializing a RandomForestClassifier, which does use n_estimators\\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)', 'print(y_train.head())', 'print(y_train.isna().sum())', '#If y_train contains missing values:\\ny_train = y_train.dropna()', \"get_ipython().system('pip install scikit-learn')\\nfrom sklearn.impute import SimpleImputer\", 'imputer = SimpleImputer(strategy=\"mean\")\\ny_train = imputer.fit_transform(y_train.values.reshape(-1, 1))', \"from sklearn.impute import SimpleImputer\\nimport numpy as np\\n\\n# Create an imputer object with a strategy of filling missing values with the mean of the column\\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\\n\\n# Fit on the training data and transform it\\nX_train_imputed = imputer.fit_transform(X_train)\\n\\n# Don't forget to transform the test data as well, using the same imputer (without fitting it again)\\nX_test_imputed = imputer.transform(X_test)\\n\\n# Now you can fit the model with the imputed data\\nmodel.fit(X_train_imputed, y_train)\", '# Drop rows with any missing values\\ndf_cleaned = df.dropna()\\n\\n# After cleaning, proceed with your feature/target selection, data splitting, and so on', 'from sklearn.experimental import enable_hist_gradient_boosting  # noqa\\nfrom sklearn.ensemble import HistGradientBoostingClassifier\\n\\n# Initialize the model that can handle NaN values\\nmodel_with_nan_support = HistGradientBoostingClassifier()\\n\\n# This model can be directly fitted with data containing NaNs\\nmodel_with_nan_support.fit(X_train, y_train)', \"get_ipython().run_line_magic('pinfo', 'model.fit')\", 'model.fit(X_train, y_train)', 'y_pred = model.predict(X_test)', \"# Assuming you've uploaded the CSV to your Google Colab session\\nheart_df = pd.read_csv('heart_disease_health_indicators.csv')\", '#Identify missing values\\nmissing_values = heart_df.info()', '#Removing the null values\\n\\ndf.isnull().sum()', \"import numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\", \"# Assuming you've uploaded the CSV to your Google Colab session\\nheart_df = pd.read_csv('heart_disease_health_indicators.csv')\", '#Identify missing values\\nmissing_values = heart_df.info()', '#Removing the null values\\n\\ndf.isnull().sum()', \"columns_with_missing_values = ['Diabetes', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income']\\ndf_cleaned_specific = df.dropna(subset=columns_with_missing_values)\", '#df.isnull().sum()', \"# Assuming there are no categorical variables to encode in this dataset\\n# Select features and target\\nX = df.drop('HeartDiseaseorAttack', axis=1)  # Features\\ny = df['HeartDiseaseorAttack']  # Target variable\", 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)', 'scaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)', \"get_ipython().run_line_magic('pip', 'install scikit-learn')\", 'from sklearn.linear_model import LogisticRegression', 'model = LogisticRegression(random_state=42)', 'from sklearn.ensemble import RandomForestClassifier\\n\\n# Example of initializing a RandomForestClassifier, which does use n_estimators\\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)', 'print(y_train.head())', 'print(y_train.isna().sum())', '#If y_train contains missing values:\\ny_train = y_train.dropna()', \"get_ipython().system('pip install scikit-learn')\\nfrom sklearn.impute import SimpleImputer\", 'imputer = SimpleImputer(strategy=\"mean\")\\ny_train = imputer.fit_transform(y_train.values.reshape(-1, 1))', \"from sklearn.impute import SimpleImputer\\nimport numpy as np\\n\\n# Create an imputer object with a strategy of filling missing values with the mean of the column\\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\\n\\n# Fit on the training data and transform it\\nX_train_imputed = imputer.fit_transform(X_train)\\n\\n# Don't forget to transform the test data as well, using the same imputer (without fitting it again)\\nX_test_imputed = imputer.transform(X_test)\\n\\n# Now you can fit the model with the imputed data\\nmodel.fit(X_train_imputed, y_train)\", '# Drop rows with any missing values\\ndf_cleaned = df.dropna()\\n\\n# After cleaning, proceed with your feature/target selection, data splitting, and so on', 'from sklearn.experimental import enable_hist_gradient_boosting  # noqa\\nfrom sklearn.ensemble import HistGradientBoostingClassifier\\n\\n# Initialize the model that can handle NaN values\\nmodel_with_nan_support = HistGradientBoostingClassifier()\\n\\n# This model can be directly fitted with data containing NaNs\\nmodel_with_nan_support.fit(X_train, y_train)', \"get_ipython().run_line_magic('pinfo', 'model.fit')\", 'model.fit(X_train, y_train)', 'y_pred = model.predict(X_test)', \"import numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\", \"import numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\", \"# Assuming you've uploaded the CSV to your Google Colab session\\nheart_df = pd.read_csv('heart_disease_health_indicators.csv')\", '#Identify missing values\\nmissing_values = heart_df.info()', '#Removing the null values\\n\\ndf.isnull().sum()', \"columns_with_missing_values = ['Diabetes', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income']\\ndf_cleaned_specific = df.dropna(subset=columns_with_missing_values)\", '#df.isnull().sum()', \"# Assuming there are no categorical variables to encode in this dataset\\n# Select features and target\\nX = df.drop('HeartDiseaseorAttack', axis=1)  # Features\\ny = df['HeartDiseaseorAttack']  # Target variable\", 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)', 'scaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)', \"get_ipython().run_line_magic('pip', 'install scikit-learn')\", 'from sklearn.linear_model import LogisticRegression', 'model = LogisticRegression(random_state=42)', 'from sklearn.ensemble import RandomForestClassifier\\n\\n# Example of initializing a RandomForestClassifier, which does use n_estimators\\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)', 'print(y_train.head())', 'print(y_train.isna().sum())', '#If y_train contains missing values:\\ny_train = y_train.dropna()', \"get_ipython().system('pip install scikit-learn')\\nfrom sklearn.impute import SimpleImputer\", 'imputer = SimpleImputer(strategy=\"mean\")\\ny_train = imputer.fit_transform(y_train.values.reshape(-1, 1))', \"from sklearn.impute import SimpleImputer\\nimport numpy as np\\n\\n# Create an imputer object with a strategy of filling missing values with the mean of the column\\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\\n\\n# Fit on the training data and transform it\\nX_train_imputed = imputer.fit_transform(X_train)\\n\\n# Don't forget to transform the test data as well, using the same imputer (without fitting it again)\\nX_test_imputed = imputer.transform(X_test)\\n\\n# Now you can fit the model with the imputed data\\nmodel.fit(X_train_imputed, y_train)\", '# Drop rows with any missing values\\ndf_cleaned = df.dropna()\\n\\n# After cleaning, proceed with your feature/target selection, data splitting, and so on', 'from sklearn.experimental import enable_hist_gradient_boosting  # noqa\\nfrom sklearn.ensemble import HistGradientBoostingClassifier\\n\\n# Initialize the model that can handle NaN values\\nmodel_with_nan_support = HistGradientBoostingClassifier()\\n\\n# This model can be directly fitted with data containing NaNs\\nmodel_with_nan_support.fit(X_train, y_train)', \"get_ipython().run_line_magic('pinfo', 'model.fit')\", 'model.fit(X_train, y_train)', 'y_pred = model.predict(X_test)', \"# Define a pipeline\\npipeline = Pipeline([\\n    ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values\\n    ('scaler', StandardScaler()),  # Scale features\\n    ('classifier', LogisticRegression(random_state=42))  # Classifier\\n])\\n\\n# Define a grid of parameters to search\\nparam_grid = {\\n    'imputer__strategy': ['mean', 'median'],  # Try different strategies for imputation\\n    'classifier__C': [0.1, 1, 10],  # Regularization strength for LogisticRegression\\n    'classifier__solver': ['liblinear', 'saga']  # Optimization algorithms\\n}\", 'y_pred = model.predict(X_test)', '# Accuracy\\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\\n\\n# Confusion Matrix\\nconf_matrix = confusion_matrix(y_test, y_pred)\\nsns.heatmap(conf_matrix, annot=True, fmt=\\'d\\')\\nplt.show()\\n\\n# Classification Report\\nprint(classification_report(y_test, y_pred))', '# After fitting the model, you can use it to make predictions or assess its performance\\ny_pred = model.predict(X_test)', '# After fitting the model, you can use it to make predictions or assess its performance\\ny_pred = model.predict(y_test)', '# After fitting the model, you can use it to make predictions or assess its performance\\n# Correct way to make predictions\\ny_pred = model.predict(X_test)', \"#Dependecies\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\", \"# Assuming you've uploaded the CSV to your Google Colab session\\nheart_df = pd.read_csv('heart_disease_health_indicators.csv')\", '#Identify missing values\\nmissing_values = heart_df.info()', '#Removing the null values\\n\\ndf.isnull().sum()', \"#Dependecies\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\", \"# Assuming you've uploaded the CSV to your Google Colab session\\nheart_df = pd.read_csv('heart_disease_health_indicators.csv')\", '#Identify missing values\\nmissing_values = heart_df.info()', '#Identify missing values\\n heart_df.info()', '#Identify missing values\\nheart_df.info()', '#Removing the null values\\n\\nheart_df.isnull().sum()', \"columns_with_missing_values = ['Diabetes', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income']\\ndf_cleaned_specific = df.dropna(subset=columns_with_missing_values)\", \"# Assuming there are no categorical variables to encode in this dataset\\n# Select features and target\\nX = df.drop('HeartDiseaseorAttack', axis=1)  # Features\\ny = df['HeartDiseaseorAttack']  # Target variable\", 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)', 'scaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)', \"get_ipython().run_line_magic('pip', 'install scikit-learn')\", 'from sklearn.linear_model import LogisticRegression', 'model = LogisticRegression(random_state=42)', 'from sklearn.ensemble import RandomForestClassifier\\n\\n# Example of initializing a RandomForestClassifier, which does use n_estimators\\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)', 'print(y_train.head())', 'print(y_train.isna().sum())', '#If y_train contains missing values:\\ny_train = y_train.dropna()', \"get_ipython().system('pip install scikit-learn')\\nfrom sklearn.impute import SimpleImputer\", 'imputer = SimpleImputer(strategy=\"mean\")\\ny_train = imputer.fit_transform(y_train.values.reshape(-1, 1))', \"from sklearn.impute import SimpleImputer\\nimport numpy as np\\n\\n# Create an imputer object with a strategy of filling missing values with the mean of the column\\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\\n\\n# Fit on the training data and transform it\\nX_train_imputed = imputer.fit_transform(X_train)\\n\\n# Don't forget to transform the test data as well, using the same imputer (without fitting it again)\\nX_test_imputed = imputer.transform(X_test)\\n\\n# Now you can fit the model with the imputed data\\nmodel.fit(X_train_imputed, y_train)\", '# Drop rows with any missing values\\ndf_cleaned = df.dropna()\\n\\n# After cleaning, proceed with your feature/target selection, data splitting, and so on', 'from sklearn.experimental import enable_hist_gradient_boosting  # noqa\\nfrom sklearn.ensemble import HistGradientBoostingClassifier\\n\\n# Initialize the model that can handle NaN values\\nmodel_with_nan_support = HistGradientBoostingClassifier()\\n\\n# This model can be directly fitted with data containing NaNs\\nmodel_with_nan_support.fit(X_train, y_train)', 'model.fit(X_train, y_train)', '# After fitting the model, you can use it to make predictions or assess its performance\\n# Correct way to make predictions\\ny_pred = model.predict(X_test)', \"# Define a pipeline\\npipeline = Pipeline([\\n    ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values\\n    ('scaler', StandardScaler()),  # Scale features\\n    ('classifier', LogisticRegression(random_state=42))  # Classifier\\n])\\n\\n# Define a grid of parameters to search\\nparam_grid = {\\n    'imputer__strategy': ['mean', 'median'],  # Try different strategies for imputation\\n    'classifier__C': [0.1, 1, 10],  # Regularization strength for LogisticRegression\\n    'classifier__solver': ['liblinear', 'saga']  # Optimization algorithms\\n}\", 'y_pred = model.predict(X_test)', '# Accuracy\\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\\n\\n# Confusion Matrix\\nconf_matrix = confusion_matrix(y_test, y_pred)\\nsns.heatmap(conf_matrix, annot=True, fmt=\\'d\\')\\nplt.show()\\n\\n# Classification Report\\nprint(classification_report(y_test, y_pred))', \"# Define a pipeline\\npipeline = Pipeline([\\n    ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values\\n    ('scaler', StandardScaler()),  # Scale features\\n    ('classifier', LogisticRegression(random_state=42))  # Classifier\\n])\\n\\n# Define a grid of parameters to search\\nparam_grid = {\\n    'imputer__strategy': ['mean', 'median'],  # Try different strategies for imputation\\n    'classifier__C': [0.1, 1, 10],  # Regularization strength for LogisticRegression\\n    'classifier__solver': ['liblinear', 'saga']  # Optimization algorithms\\n}\\n\\ny_pred = model.predict(X_test)\", \"# Define a pipeline\\npipeline = Pipeline([\\n    ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values\\n    ('scaler', StandardScaler()),  # Scale features\\n    ('classifier', LogisticRegression(random_state=42))  # Classifier\\n])\\n\\n# Define a grid of parameters to search\\nparam_grid = {\\n    'imputer__strategy': ['mean', 'median'],  # Try different strategies for imputation\\n    'classifier__C': [0.1, 1, 10],  # Regularization strength for LogisticRegression\\n    'classifier__solver': ['liblinear', 'saga']  # Optimization algorithms\\n}\", '# Fit the pipeline on the training data\\npipeline.fit(X_train, y_train)\\n\\n# Predict on the test data using the pipeline\\ny_pred = pipeline.predict(X_test)', '# Accuracy\\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\\n\\n# Confusion Matrix\\nconf_matrix = confusion_matrix(y_test, y_pred)\\nsns.heatmap(conf_matrix, annot=True, fmt=\\'d\\')\\nplt.show()\\n\\n# Classification Report\\nprint(classification_report(y_test, y_pred))', \"# Define a grid of parameters to search\\nparam_grid = {\\n    'imputer__strategy': ['mean', 'median'],  # Try different strategies for imputation\\n    'classifier__C': [0.1, 1, 10],  # Regularization strength for LogisticRegression\\n    'classifier__solver': ['liblinear', 'saga']  # Optimization algorithms\\n}\", 'from sklearn.model_selection import GridSearchCV', '# Define the pipeline and parameter grid\\npipeline = ...\\nparam_grid = ...', \"# Instantiate the GridSearchCV object\\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1)\", 'import pandas as pd', 'print(X_train.head())\\nprint(y_train.head())', 'X_train = pd.DataFrame(X_train)\\ny_train = pd.DataFrame(y_train)', 'print(X_train.head())\\nprint(y_train.head())', \"from google.colab import drive\\ndrive.mount('/content/drive')\", '# Fit the GridSearchCV object\\ngrid_search.fit(X_train_scaled, y_train)', 'from sklearn.model_selection import GridSearchCV', 'X_train_scaled = # Load the scaled training data', '# After fitting the model, you can use it to make predictions or assess its performance\\n# Correct way to make predictions\\ny_pred = model.predict(X_test)', 'X_train_scaled = X_train_scaled(scaler.fit_transform(X_train))', 'X_train_scaled = X_train_scaled(scaler.fit_transform(X_train_scaled))', 'X_train_scaled = X_train_scaled(scaler.fit_transform(X_train_scaled))\\nX_train_scaled.df()', 'print(X_train_scaled)', 'X_train_scaled = X_train_scaled(scaler.fit_transform(X_train_scaled))\\nprint(X_train_scaled)', 'print(X_train_scaled)', 'X_train_scaled = X_train_scaled(scaler.fit_transform(X_train_scaled))\\nX_train_scaled.df()', 'X_train= X_train(scaler.fit_transform(X_train))\\nX_train.df()', 'print(X_train)', 'X_train= X_train(scaler.fit_transform(X_train))\\nX_train.df()', 'X_train= X_train(scaler.fit_transform(X_train))\\nX_train.head()', 'X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=df.columns)\\nX_train_scaled.head()', 'print(X_train.shape)\\nprint(df.columns.shape)', 'num_cols_transformed = X_train_scaled.shape[1]\\nnum_cols_original = df.columns.shape[0]\\n\\nprint(f\"Number of columns in transformed data: {num_cols_transformed}\")\\nprint(f\"Number of columns in original DataFrame: {num_cols_original}\")', 'num_cols_transformed = X_train.shape[1]\\nnum_cols_original = df.columns.shape[0]\\n\\nprint(f\"Number of columns in transformed data: {num_cols_transformed}\")\\nprint(f\"Number of columns in original DataFrame: {num_cols_original}\")', 'X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=df.columns)\\nX_train_scaled.head()', 'X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)', 'X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=df.columns)', '# Fit the GridSearchCV object\\ngrid_search.fit(X_train_scaled, y_train)', '# Fit the GridSearchCV object\\ngrid_search.fit(X_train, y_train)', 'print(locals())'], '_oh': {27: HeartDiseaseorAttack    0\n",
            "HighBP                  0\n",
            "HighChol                0\n",
            "CholCheck               0\n",
            "BMI                     0\n",
            "Smoker                  0\n",
            "Stroke                  0\n",
            "Diabetes                0\n",
            "PhysActivity            0\n",
            "Fruits                  0\n",
            "Veggies                 0\n",
            "HvyAlcoholConsump       0\n",
            "AnyHealthcare           0\n",
            "NoDocbcCost             1\n",
            "GenHlth                 1\n",
            "MentHlth                1\n",
            "PhysHlth                1\n",
            "DiffWalk                1\n",
            "Sex                     1\n",
            "Age                     1\n",
            "Education               1\n",
            "Income                  1\n",
            "dtype: int64, 30: HeartDiseaseorAttack    0\n",
            "HighBP                  0\n",
            "HighChol                0\n",
            "CholCheck               0\n",
            "BMI                     0\n",
            "Smoker                  0\n",
            "Stroke                  0\n",
            "Diabetes                0\n",
            "PhysActivity            0\n",
            "Fruits                  0\n",
            "Veggies                 0\n",
            "HvyAlcoholConsump       0\n",
            "AnyHealthcare           0\n",
            "NoDocbcCost             1\n",
            "GenHlth                 1\n",
            "MentHlth                1\n",
            "PhysHlth                1\n",
            "DiffWalk                1\n",
            "Sex                     1\n",
            "Age                     1\n",
            "Education               1\n",
            "Income                  1\n",
            "dtype: int64, 35: HeartDiseaseorAttack    0\n",
            "HighBP                  0\n",
            "HighChol                0\n",
            "CholCheck               0\n",
            "BMI                     0\n",
            "Smoker                  0\n",
            "Stroke                  0\n",
            "Diabetes                0\n",
            "PhysActivity            0\n",
            "Fruits                  0\n",
            "Veggies                 0\n",
            "HvyAlcoholConsump       0\n",
            "AnyHealthcare           0\n",
            "NoDocbcCost             1\n",
            "GenHlth                 1\n",
            "MentHlth                1\n",
            "PhysHlth                1\n",
            "DiffWalk                1\n",
            "Sex                     1\n",
            "Age                     1\n",
            "Education               1\n",
            "Income                  1\n",
            "dtype: int64, 50: RandomForestClassifier(random_state=42), 52: HistGradientBoostingClassifier(), 54: RandomForestClassifier(random_state=42), 58: HeartDiseaseorAttack    0\n",
            "HighBP                  0\n",
            "HighChol                0\n",
            "CholCheck               0\n",
            "BMI                     0\n",
            "Smoker                  0\n",
            "Stroke                  0\n",
            "Diabetes                0\n",
            "PhysActivity            0\n",
            "Fruits                  0\n",
            "Veggies                 0\n",
            "HvyAlcoholConsump       0\n",
            "AnyHealthcare           0\n",
            "NoDocbcCost             1\n",
            "GenHlth                 1\n",
            "MentHlth                1\n",
            "PhysHlth                1\n",
            "DiffWalk                1\n",
            "Sex                     1\n",
            "Age                     1\n",
            "Education               1\n",
            "Income                  1\n",
            "dtype: int64, 62: HeartDiseaseorAttack    0\n",
            "HighBP                  0\n",
            "HighChol                0\n",
            "CholCheck               0\n",
            "BMI                     0\n",
            "Smoker                  0\n",
            "Stroke                  0\n",
            "Diabetes                0\n",
            "PhysActivity            0\n",
            "Fruits                  0\n",
            "Veggies                 0\n",
            "HvyAlcoholConsump       0\n",
            "AnyHealthcare           0\n",
            "NoDocbcCost             1\n",
            "GenHlth                 1\n",
            "MentHlth                1\n",
            "PhysHlth                1\n",
            "DiffWalk                1\n",
            "Sex                     1\n",
            "Age                     1\n",
            "Education               1\n",
            "Income                  1\n",
            "dtype: int64, 77: RandomForestClassifier(random_state=42), 79: HistGradientBoostingClassifier(), 81: RandomForestClassifier(random_state=42), 87: HeartDiseaseorAttack    0\n",
            "HighBP                  0\n",
            "HighChol                0\n",
            "CholCheck               0\n",
            "BMI                     0\n",
            "Smoker                  0\n",
            "Stroke                  0\n",
            "Diabetes                0\n",
            "PhysActivity            0\n",
            "Fruits                  0\n",
            "Veggies                 0\n",
            "HvyAlcoholConsump       0\n",
            "AnyHealthcare           0\n",
            "NoDocbcCost             1\n",
            "GenHlth                 1\n",
            "MentHlth                1\n",
            "PhysHlth                1\n",
            "DiffWalk                1\n",
            "Sex                     1\n",
            "Age                     1\n",
            "Education               1\n",
            "Income                  1\n",
            "dtype: int64, 102: RandomForestClassifier(random_state=42), 104: HistGradientBoostingClassifier(), 106: RandomForestClassifier(random_state=42), 117: HeartDiseaseorAttack    0\n",
            "HighBP                  0\n",
            "HighChol                0\n",
            "CholCheck               0\n",
            "BMI                     0\n",
            "Smoker                  0\n",
            "Stroke                  0\n",
            "Diabetes                0\n",
            "PhysActivity            0\n",
            "Fruits                  0\n",
            "Veggies                 0\n",
            "HvyAlcoholConsump       0\n",
            "AnyHealthcare           0\n",
            "NoDocbcCost             1\n",
            "GenHlth                 1\n",
            "MentHlth                1\n",
            "PhysHlth                1\n",
            "DiffWalk                1\n",
            "Sex                     1\n",
            "Age                     1\n",
            "Education               1\n",
            "Income                  1\n",
            "dtype: int64, 123: HeartDiseaseorAttack    0\n",
            "HighBP                  0\n",
            "HighChol                0\n",
            "CholCheck               0\n",
            "BMI                     0\n",
            "Smoker                  0\n",
            "Stroke                  0\n",
            "Diabetes                0\n",
            "PhysActivity            0\n",
            "Fruits                  0\n",
            "Veggies                 0\n",
            "HvyAlcoholConsump       0\n",
            "AnyHealthcare           0\n",
            "NoDocbcCost             0\n",
            "GenHlth                 0\n",
            "MentHlth                0\n",
            "PhysHlth                0\n",
            "DiffWalk                0\n",
            "Sex                     0\n",
            "Age                     0\n",
            "Education               0\n",
            "Income                  0\n",
            "dtype: int64, 137: RandomForestClassifier(random_state=42), 139: HistGradientBoostingClassifier(), 140: RandomForestClassifier(random_state=42)}, '_dh': ['/content'], 'In': ['', \"# Define a grid of parameters to search\\nparam_grid = {\\n    'imputer__strategy': ['mean', 'median'],  # Try different strategies for imputation\\n    'classifier__C': [0.1, 1, 10],  # Regularization strength for LogisticRegression\\n    'classifier__solver': ['liblinear', 'saga']  # Optimization algorithms\\n}\", '# Set up GridSearchCV\\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\\'accuracy\\', verbose=1)\\n\\n# Fit GridSearchCV\\ngrid_search.fit(X_train_scaled, y_train)\\n\\n# Best parameters found\\nprint(\"Best parameters:\", grid_search.best_params_)\\n\\n# Evaluate on the test set\\ny_pred = grid_search.predict(X_test_scaled)\\nprint(classification_report(y_test, y_pred))', '# Set up GridSearchCV\\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\\'accuracy\\', verbose=1)\\ngrid_search.df\\n\\n# Fit GridSearchCV\\ngrid_search.fit(X_train_scaled, y_train)\\n\\n# Best parameters found\\nprint(\"Best parameters:\", grid_search.best_params_)\\n\\n# Evaluate on the test set\\ny_pred = grid_search.predict(X_test_scaled)\\nprint(classification_report(y_test, y_pred))', 'from sklearn.model_selection import GridSearchCV', '# Define the pipeline and parameter grid\\npipeline = ...\\nparam_grid = ...', \"# Instantiate the GridSearchCV object\\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1)\", '# Fit the GridSearchCV object\\ngrid_search.fit(X_train_scaled, y_train)', '# Fit the GridSearchCV object\\ngrid_search.fit(X_train, y_train)', '# Fit the GridSearchCV object\\ngrid_search.fit(X_train_scaled, y_train)\\nX-train_scaled.df', '# Fit the GridSearchCV object\\ngrid_search.fit(X_train_scaled, y_train)', '# Load the data into X_train_scaled and y_train variables\\n# Replace the following lines with your actual data loading code\\nX_train_scaled = pd.read_csv(\"X_train_scaled.csv\")\\ny_train = pd.read_csv(\"y_train.csv\")', 'import pandas as pd', '# Load the data into X_train_scaled and y_train variables\\n# Replace the following lines with your actual data loading code\\nX_train_scaled = pd.read_csv(\"X_train_scaled.csv\")\\ny_train = pd.read_csv(\"y_train.csv\")', '# Load the data into X_train_scaled and y_train variables\\n# Replace the following lines with your actual data loading code\\nX_train_scaled = pd.read_csv(\"heart_disease_health_indicators.csv\")\\ny_train = pd.read_csv(\"heart_disease_health_indicators.csv\")', '# Load the data into X_train_scaled and y_train variables\\n# Replace the following lines with your actual data loading code\\nX_train_scaled = pd.read_csv(\"heart_disease_health_indicators.csv\")\\ny_train = pd.read_csv(\"heart_disease_health_indicators.csv\")', 'print(X_train_scaled.head())\\nprint(y_train.head())', '# Load the data into X_train_scaled and y_train variables\\n# Replace the following lines with your actual data loading code\\nX_train = pd.read_csv(\"heart_disease_health_indicators.csv\")\\ny_train = pd.read_csv(\"heart_disease_health_indicators.csv\")', '# Load the data into X_train_scaled and y_train variables\\n# Replace the following lines with your actual data loading code\\nX_train = pd.read_csv(\"/Resources/heart_disease_health_indicators.csv\")\\ny_train = pd.read_csv(\"Resources/heart_disease_health_indicators.csv\")', 'print(X_train.head())\\nprint(y_train.head())', 'print(X_train.head())\\nprint(y_train.head())', \"import numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\", \"# Assuming you've uploaded the CSV to your Google Colab session\\nheart_df = pd.read_csv('heart_disease_health_indicators.csv')\", '#Identify missing values\\nmissing_values = heart_df.info()', \"# Assuming you've uploaded the CSV to your Google Colab session\\ndf = pd.read_csv('/content/heart_disease_health_indicators.csv')\", \"# Assuming you've uploaded the CSV to your Google Colab session\\ndf = pd.read_csv('/content/heart_disease_health_indicators.csv')\", '#Identify missing values\\nmissing_values = heart_df.info()', '#Removing the null values\\n\\ndf.isnull().sum()', \"# Assuming you've uploaded the CSV to your Google Colab session\\nheart_df = pd.read_csv('/content/heart_disease_health_indicators.csv')\", '#Identify missing values\\nmissing_values = heart_df.info()', '#Removing the null values\\n\\ndf.isnull().sum()', \"columns_with_missing_values = ['Diabetes', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income']\\ndf_cleaned_specific = df.dropna(subset=columns_with_missing_values)\", \"import numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\", \"# Assuming you've uploaded the CSV to your Google Colab session\\nheart_df = pd.read_csv('/content/heart_disease_health_indicators.csv')\", '#Identify missing values\\nmissing_values = heart_df.info()', '#Removing the null values\\n\\ndf.isnull().sum()', \"columns_with_missing_values = ['Diabetes', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income']\\ndf_cleaned_specific = df.dropna(subset=columns_with_missing_values)\", '#df.isnull().sum()', \"# Assuming there are no categorical variables to encode in this dataset\\n# Select features and target\\nX = df.drop('HeartDiseaseorAttack', axis=1)  # Features\\ny = df['HeartDiseaseorAttack']  # Target variable\", 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)', 'scaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)', \"get_ipython().run_line_magic('pip', 'install scikit-learn')\", 'from sklearn.linear_model import LogisticRegression', 'model = LogisticRegression(random_state=42)', 'from sklearn.ensemble import RandomForestClassifier\\n\\n# Example of initializing a RandomForestClassifier, which does use n_estimators\\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)', 'print(y_train.head())', 'print(y_train.isna().sum())', '#If y_train contains missing values:\\ny_train = y_train.dropna()', \"get_ipython().system('pip install scikit-learn')\\nfrom sklearn.impute import SimpleImputer\", 'imputer = SimpleImputer(strategy=\"mean\")\\ny_train = imputer.fit_transform(y_train.values.reshape(-1, 1))', \"from sklearn.impute import SimpleImputer\\nimport numpy as np\\n\\n# Create an imputer object with a strategy of filling missing values with the mean of the column\\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\\n\\n# Fit on the training data and transform it\\nX_train_imputed = imputer.fit_transform(X_train)\\n\\n# Don't forget to transform the test data as well, using the same imputer (without fitting it again)\\nX_test_imputed = imputer.transform(X_test)\\n\\n# Now you can fit the model with the imputed data\\nmodel.fit(X_train_imputed, y_train)\", '# Drop rows with any missing values\\ndf_cleaned = df.dropna()\\n\\n# After cleaning, proceed with your feature/target selection, data splitting, and so on', 'from sklearn.experimental import enable_hist_gradient_boosting  # noqa\\nfrom sklearn.ensemble import HistGradientBoostingClassifier\\n\\n# Initialize the model that can handle NaN values\\nmodel_with_nan_support = HistGradientBoostingClassifier()\\n\\n# This model can be directly fitted with data containing NaNs\\nmodel_with_nan_support.fit(X_train, y_train)', \"get_ipython().run_line_magic('pinfo', 'model.fit')\", 'model.fit(X_train, y_train)', 'y_pred = model.predict(X_test)', \"# Assuming you've uploaded the CSV to your Google Colab session\\nheart_df = pd.read_csv('heart_disease_health_indicators.csv')\", '#Identify missing values\\nmissing_values = heart_df.info()', '#Removing the null values\\n\\ndf.isnull().sum()', \"import numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\", \"# Assuming you've uploaded the CSV to your Google Colab session\\nheart_df = pd.read_csv('heart_disease_health_indicators.csv')\", '#Identify missing values\\nmissing_values = heart_df.info()', '#Removing the null values\\n\\ndf.isnull().sum()', \"columns_with_missing_values = ['Diabetes', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income']\\ndf_cleaned_specific = df.dropna(subset=columns_with_missing_values)\", '#df.isnull().sum()', \"# Assuming there are no categorical variables to encode in this dataset\\n# Select features and target\\nX = df.drop('HeartDiseaseorAttack', axis=1)  # Features\\ny = df['HeartDiseaseorAttack']  # Target variable\", 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)', 'scaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)', \"get_ipython().run_line_magic('pip', 'install scikit-learn')\", 'from sklearn.linear_model import LogisticRegression', 'model = LogisticRegression(random_state=42)', 'from sklearn.ensemble import RandomForestClassifier\\n\\n# Example of initializing a RandomForestClassifier, which does use n_estimators\\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)', 'print(y_train.head())', 'print(y_train.isna().sum())', '#If y_train contains missing values:\\ny_train = y_train.dropna()', \"get_ipython().system('pip install scikit-learn')\\nfrom sklearn.impute import SimpleImputer\", 'imputer = SimpleImputer(strategy=\"mean\")\\ny_train = imputer.fit_transform(y_train.values.reshape(-1, 1))', \"from sklearn.impute import SimpleImputer\\nimport numpy as np\\n\\n# Create an imputer object with a strategy of filling missing values with the mean of the column\\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\\n\\n# Fit on the training data and transform it\\nX_train_imputed = imputer.fit_transform(X_train)\\n\\n# Don't forget to transform the test data as well, using the same imputer (without fitting it again)\\nX_test_imputed = imputer.transform(X_test)\\n\\n# Now you can fit the model with the imputed data\\nmodel.fit(X_train_imputed, y_train)\", '# Drop rows with any missing values\\ndf_cleaned = df.dropna()\\n\\n# After cleaning, proceed with your feature/target selection, data splitting, and so on', 'from sklearn.experimental import enable_hist_gradient_boosting  # noqa\\nfrom sklearn.ensemble import HistGradientBoostingClassifier\\n\\n# Initialize the model that can handle NaN values\\nmodel_with_nan_support = HistGradientBoostingClassifier()\\n\\n# This model can be directly fitted with data containing NaNs\\nmodel_with_nan_support.fit(X_train, y_train)', \"get_ipython().run_line_magic('pinfo', 'model.fit')\", 'model.fit(X_train, y_train)', 'y_pred = model.predict(X_test)', \"import numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\", \"import numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\", \"# Assuming you've uploaded the CSV to your Google Colab session\\nheart_df = pd.read_csv('heart_disease_health_indicators.csv')\", '#Identify missing values\\nmissing_values = heart_df.info()', '#Removing the null values\\n\\ndf.isnull().sum()', \"columns_with_missing_values = ['Diabetes', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income']\\ndf_cleaned_specific = df.dropna(subset=columns_with_missing_values)\", '#df.isnull().sum()', \"# Assuming there are no categorical variables to encode in this dataset\\n# Select features and target\\nX = df.drop('HeartDiseaseorAttack', axis=1)  # Features\\ny = df['HeartDiseaseorAttack']  # Target variable\", 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)', 'scaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)', \"get_ipython().run_line_magic('pip', 'install scikit-learn')\", 'from sklearn.linear_model import LogisticRegression', 'model = LogisticRegression(random_state=42)', 'from sklearn.ensemble import RandomForestClassifier\\n\\n# Example of initializing a RandomForestClassifier, which does use n_estimators\\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)', 'print(y_train.head())', 'print(y_train.isna().sum())', '#If y_train contains missing values:\\ny_train = y_train.dropna()', \"get_ipython().system('pip install scikit-learn')\\nfrom sklearn.impute import SimpleImputer\", 'imputer = SimpleImputer(strategy=\"mean\")\\ny_train = imputer.fit_transform(y_train.values.reshape(-1, 1))', \"from sklearn.impute import SimpleImputer\\nimport numpy as np\\n\\n# Create an imputer object with a strategy of filling missing values with the mean of the column\\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\\n\\n# Fit on the training data and transform it\\nX_train_imputed = imputer.fit_transform(X_train)\\n\\n# Don't forget to transform the test data as well, using the same imputer (without fitting it again)\\nX_test_imputed = imputer.transform(X_test)\\n\\n# Now you can fit the model with the imputed data\\nmodel.fit(X_train_imputed, y_train)\", '# Drop rows with any missing values\\ndf_cleaned = df.dropna()\\n\\n# After cleaning, proceed with your feature/target selection, data splitting, and so on', 'from sklearn.experimental import enable_hist_gradient_boosting  # noqa\\nfrom sklearn.ensemble import HistGradientBoostingClassifier\\n\\n# Initialize the model that can handle NaN values\\nmodel_with_nan_support = HistGradientBoostingClassifier()\\n\\n# This model can be directly fitted with data containing NaNs\\nmodel_with_nan_support.fit(X_train, y_train)', \"get_ipython().run_line_magic('pinfo', 'model.fit')\", 'model.fit(X_train, y_train)', 'y_pred = model.predict(X_test)', \"# Define a pipeline\\npipeline = Pipeline([\\n    ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values\\n    ('scaler', StandardScaler()),  # Scale features\\n    ('classifier', LogisticRegression(random_state=42))  # Classifier\\n])\\n\\n# Define a grid of parameters to search\\nparam_grid = {\\n    'imputer__strategy': ['mean', 'median'],  # Try different strategies for imputation\\n    'classifier__C': [0.1, 1, 10],  # Regularization strength for LogisticRegression\\n    'classifier__solver': ['liblinear', 'saga']  # Optimization algorithms\\n}\", 'y_pred = model.predict(X_test)', '# Accuracy\\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\\n\\n# Confusion Matrix\\nconf_matrix = confusion_matrix(y_test, y_pred)\\nsns.heatmap(conf_matrix, annot=True, fmt=\\'d\\')\\nplt.show()\\n\\n# Classification Report\\nprint(classification_report(y_test, y_pred))', '# After fitting the model, you can use it to make predictions or assess its performance\\ny_pred = model.predict(X_test)', '# After fitting the model, you can use it to make predictions or assess its performance\\ny_pred = model.predict(y_test)', '# After fitting the model, you can use it to make predictions or assess its performance\\n# Correct way to make predictions\\ny_pred = model.predict(X_test)', \"#Dependecies\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\", \"# Assuming you've uploaded the CSV to your Google Colab session\\nheart_df = pd.read_csv('heart_disease_health_indicators.csv')\", '#Identify missing values\\nmissing_values = heart_df.info()', '#Removing the null values\\n\\ndf.isnull().sum()', \"#Dependecies\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\", \"# Assuming you've uploaded the CSV to your Google Colab session\\nheart_df = pd.read_csv('heart_disease_health_indicators.csv')\", '#Identify missing values\\nmissing_values = heart_df.info()', '#Identify missing values\\n heart_df.info()', '#Identify missing values\\nheart_df.info()', '#Removing the null values\\n\\nheart_df.isnull().sum()', \"columns_with_missing_values = ['Diabetes', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income']\\ndf_cleaned_specific = df.dropna(subset=columns_with_missing_values)\", \"# Assuming there are no categorical variables to encode in this dataset\\n# Select features and target\\nX = df.drop('HeartDiseaseorAttack', axis=1)  # Features\\ny = df['HeartDiseaseorAttack']  # Target variable\", 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)', 'scaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)', \"get_ipython().run_line_magic('pip', 'install scikit-learn')\", 'from sklearn.linear_model import LogisticRegression', 'model = LogisticRegression(random_state=42)', 'from sklearn.ensemble import RandomForestClassifier\\n\\n# Example of initializing a RandomForestClassifier, which does use n_estimators\\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)', 'print(y_train.head())', 'print(y_train.isna().sum())', '#If y_train contains missing values:\\ny_train = y_train.dropna()', \"get_ipython().system('pip install scikit-learn')\\nfrom sklearn.impute import SimpleImputer\", 'imputer = SimpleImputer(strategy=\"mean\")\\ny_train = imputer.fit_transform(y_train.values.reshape(-1, 1))', \"from sklearn.impute import SimpleImputer\\nimport numpy as np\\n\\n# Create an imputer object with a strategy of filling missing values with the mean of the column\\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\\n\\n# Fit on the training data and transform it\\nX_train_imputed = imputer.fit_transform(X_train)\\n\\n# Don't forget to transform the test data as well, using the same imputer (without fitting it again)\\nX_test_imputed = imputer.transform(X_test)\\n\\n# Now you can fit the model with the imputed data\\nmodel.fit(X_train_imputed, y_train)\", '# Drop rows with any missing values\\ndf_cleaned = df.dropna()\\n\\n# After cleaning, proceed with your feature/target selection, data splitting, and so on', 'from sklearn.experimental import enable_hist_gradient_boosting  # noqa\\nfrom sklearn.ensemble import HistGradientBoostingClassifier\\n\\n# Initialize the model that can handle NaN values\\nmodel_with_nan_support = HistGradientBoostingClassifier()\\n\\n# This model can be directly fitted with data containing NaNs\\nmodel_with_nan_support.fit(X_train, y_train)', 'model.fit(X_train, y_train)', '# After fitting the model, you can use it to make predictions or assess its performance\\n# Correct way to make predictions\\ny_pred = model.predict(X_test)', \"# Define a pipeline\\npipeline = Pipeline([\\n    ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values\\n    ('scaler', StandardScaler()),  # Scale features\\n    ('classifier', LogisticRegression(random_state=42))  # Classifier\\n])\\n\\n# Define a grid of parameters to search\\nparam_grid = {\\n    'imputer__strategy': ['mean', 'median'],  # Try different strategies for imputation\\n    'classifier__C': [0.1, 1, 10],  # Regularization strength for LogisticRegression\\n    'classifier__solver': ['liblinear', 'saga']  # Optimization algorithms\\n}\", 'y_pred = model.predict(X_test)', '# Accuracy\\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\\n\\n# Confusion Matrix\\nconf_matrix = confusion_matrix(y_test, y_pred)\\nsns.heatmap(conf_matrix, annot=True, fmt=\\'d\\')\\nplt.show()\\n\\n# Classification Report\\nprint(classification_report(y_test, y_pred))', \"# Define a pipeline\\npipeline = Pipeline([\\n    ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values\\n    ('scaler', StandardScaler()),  # Scale features\\n    ('classifier', LogisticRegression(random_state=42))  # Classifier\\n])\\n\\n# Define a grid of parameters to search\\nparam_grid = {\\n    'imputer__strategy': ['mean', 'median'],  # Try different strategies for imputation\\n    'classifier__C': [0.1, 1, 10],  # Regularization strength for LogisticRegression\\n    'classifier__solver': ['liblinear', 'saga']  # Optimization algorithms\\n}\\n\\ny_pred = model.predict(X_test)\", \"# Define a pipeline\\npipeline = Pipeline([\\n    ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values\\n    ('scaler', StandardScaler()),  # Scale features\\n    ('classifier', LogisticRegression(random_state=42))  # Classifier\\n])\\n\\n# Define a grid of parameters to search\\nparam_grid = {\\n    'imputer__strategy': ['mean', 'median'],  # Try different strategies for imputation\\n    'classifier__C': [0.1, 1, 10],  # Regularization strength for LogisticRegression\\n    'classifier__solver': ['liblinear', 'saga']  # Optimization algorithms\\n}\", '# Fit the pipeline on the training data\\npipeline.fit(X_train, y_train)\\n\\n# Predict on the test data using the pipeline\\ny_pred = pipeline.predict(X_test)', '# Accuracy\\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\\n\\n# Confusion Matrix\\nconf_matrix = confusion_matrix(y_test, y_pred)\\nsns.heatmap(conf_matrix, annot=True, fmt=\\'d\\')\\nplt.show()\\n\\n# Classification Report\\nprint(classification_report(y_test, y_pred))', \"# Define a grid of parameters to search\\nparam_grid = {\\n    'imputer__strategy': ['mean', 'median'],  # Try different strategies for imputation\\n    'classifier__C': [0.1, 1, 10],  # Regularization strength for LogisticRegression\\n    'classifier__solver': ['liblinear', 'saga']  # Optimization algorithms\\n}\", 'from sklearn.model_selection import GridSearchCV', '# Define the pipeline and parameter grid\\npipeline = ...\\nparam_grid = ...', \"# Instantiate the GridSearchCV object\\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1)\", 'import pandas as pd', 'print(X_train.head())\\nprint(y_train.head())', 'X_train = pd.DataFrame(X_train)\\ny_train = pd.DataFrame(y_train)', 'print(X_train.head())\\nprint(y_train.head())', \"from google.colab import drive\\ndrive.mount('/content/drive')\", '# Fit the GridSearchCV object\\ngrid_search.fit(X_train_scaled, y_train)', 'from sklearn.model_selection import GridSearchCV', 'X_train_scaled = # Load the scaled training data', '# After fitting the model, you can use it to make predictions or assess its performance\\n# Correct way to make predictions\\ny_pred = model.predict(X_test)', 'X_train_scaled = X_train_scaled(scaler.fit_transform(X_train))', 'X_train_scaled = X_train_scaled(scaler.fit_transform(X_train_scaled))', 'X_train_scaled = X_train_scaled(scaler.fit_transform(X_train_scaled))\\nX_train_scaled.df()', 'print(X_train_scaled)', 'X_train_scaled = X_train_scaled(scaler.fit_transform(X_train_scaled))\\nprint(X_train_scaled)', 'print(X_train_scaled)', 'X_train_scaled = X_train_scaled(scaler.fit_transform(X_train_scaled))\\nX_train_scaled.df()', 'X_train= X_train(scaler.fit_transform(X_train))\\nX_train.df()', 'print(X_train)', 'X_train= X_train(scaler.fit_transform(X_train))\\nX_train.df()', 'X_train= X_train(scaler.fit_transform(X_train))\\nX_train.head()', 'X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=df.columns)\\nX_train_scaled.head()', 'print(X_train.shape)\\nprint(df.columns.shape)', 'num_cols_transformed = X_train_scaled.shape[1]\\nnum_cols_original = df.columns.shape[0]\\n\\nprint(f\"Number of columns in transformed data: {num_cols_transformed}\")\\nprint(f\"Number of columns in original DataFrame: {num_cols_original}\")', 'num_cols_transformed = X_train.shape[1]\\nnum_cols_original = df.columns.shape[0]\\n\\nprint(f\"Number of columns in transformed data: {num_cols_transformed}\")\\nprint(f\"Number of columns in original DataFrame: {num_cols_original}\")', 'X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=df.columns)\\nX_train_scaled.head()', 'X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)', 'X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=df.columns)', '# Fit the GridSearchCV object\\ngrid_search.fit(X_train_scaled, y_train)', '# Fit the GridSearchCV object\\ngrid_search.fit(X_train, y_train)', 'print(locals())'], 'Out': {27: HeartDiseaseorAttack    0\n",
            "HighBP                  0\n",
            "HighChol                0\n",
            "CholCheck               0\n",
            "BMI                     0\n",
            "Smoker                  0\n",
            "Stroke                  0\n",
            "Diabetes                0\n",
            "PhysActivity            0\n",
            "Fruits                  0\n",
            "Veggies                 0\n",
            "HvyAlcoholConsump       0\n",
            "AnyHealthcare           0\n",
            "NoDocbcCost             1\n",
            "GenHlth                 1\n",
            "MentHlth                1\n",
            "PhysHlth                1\n",
            "DiffWalk                1\n",
            "Sex                     1\n",
            "Age                     1\n",
            "Education               1\n",
            "Income                  1\n",
            "dtype: int64, 30: HeartDiseaseorAttack    0\n",
            "HighBP                  0\n",
            "HighChol                0\n",
            "CholCheck               0\n",
            "BMI                     0\n",
            "Smoker                  0\n",
            "Stroke                  0\n",
            "Diabetes                0\n",
            "PhysActivity            0\n",
            "Fruits                  0\n",
            "Veggies                 0\n",
            "HvyAlcoholConsump       0\n",
            "AnyHealthcare           0\n",
            "NoDocbcCost             1\n",
            "GenHlth                 1\n",
            "MentHlth                1\n",
            "PhysHlth                1\n",
            "DiffWalk                1\n",
            "Sex                     1\n",
            "Age                     1\n",
            "Education               1\n",
            "Income                  1\n",
            "dtype: int64, 35: HeartDiseaseorAttack    0\n",
            "HighBP                  0\n",
            "HighChol                0\n",
            "CholCheck               0\n",
            "BMI                     0\n",
            "Smoker                  0\n",
            "Stroke                  0\n",
            "Diabetes                0\n",
            "PhysActivity            0\n",
            "Fruits                  0\n",
            "Veggies                 0\n",
            "HvyAlcoholConsump       0\n",
            "AnyHealthcare           0\n",
            "NoDocbcCost             1\n",
            "GenHlth                 1\n",
            "MentHlth                1\n",
            "PhysHlth                1\n",
            "DiffWalk                1\n",
            "Sex                     1\n",
            "Age                     1\n",
            "Education               1\n",
            "Income                  1\n",
            "dtype: int64, 50: RandomForestClassifier(random_state=42), 52: HistGradientBoostingClassifier(), 54: RandomForestClassifier(random_state=42), 58: HeartDiseaseorAttack    0\n",
            "HighBP                  0\n",
            "HighChol                0\n",
            "CholCheck               0\n",
            "BMI                     0\n",
            "Smoker                  0\n",
            "Stroke                  0\n",
            "Diabetes                0\n",
            "PhysActivity            0\n",
            "Fruits                  0\n",
            "Veggies                 0\n",
            "HvyAlcoholConsump       0\n",
            "AnyHealthcare           0\n",
            "NoDocbcCost             1\n",
            "GenHlth                 1\n",
            "MentHlth                1\n",
            "PhysHlth                1\n",
            "DiffWalk                1\n",
            "Sex                     1\n",
            "Age                     1\n",
            "Education               1\n",
            "Income                  1\n",
            "dtype: int64, 62: HeartDiseaseorAttack    0\n",
            "HighBP                  0\n",
            "HighChol                0\n",
            "CholCheck               0\n",
            "BMI                     0\n",
            "Smoker                  0\n",
            "Stroke                  0\n",
            "Diabetes                0\n",
            "PhysActivity            0\n",
            "Fruits                  0\n",
            "Veggies                 0\n",
            "HvyAlcoholConsump       0\n",
            "AnyHealthcare           0\n",
            "NoDocbcCost             1\n",
            "GenHlth                 1\n",
            "MentHlth                1\n",
            "PhysHlth                1\n",
            "DiffWalk                1\n",
            "Sex                     1\n",
            "Age                     1\n",
            "Education               1\n",
            "Income                  1\n",
            "dtype: int64, 77: RandomForestClassifier(random_state=42), 79: HistGradientBoostingClassifier(), 81: RandomForestClassifier(random_state=42), 87: HeartDiseaseorAttack    0\n",
            "HighBP                  0\n",
            "HighChol                0\n",
            "CholCheck               0\n",
            "BMI                     0\n",
            "Smoker                  0\n",
            "Stroke                  0\n",
            "Diabetes                0\n",
            "PhysActivity            0\n",
            "Fruits                  0\n",
            "Veggies                 0\n",
            "HvyAlcoholConsump       0\n",
            "AnyHealthcare           0\n",
            "NoDocbcCost             1\n",
            "GenHlth                 1\n",
            "MentHlth                1\n",
            "PhysHlth                1\n",
            "DiffWalk                1\n",
            "Sex                     1\n",
            "Age                     1\n",
            "Education               1\n",
            "Income                  1\n",
            "dtype: int64, 102: RandomForestClassifier(random_state=42), 104: HistGradientBoostingClassifier(), 106: RandomForestClassifier(random_state=42), 117: HeartDiseaseorAttack    0\n",
            "HighBP                  0\n",
            "HighChol                0\n",
            "CholCheck               0\n",
            "BMI                     0\n",
            "Smoker                  0\n",
            "Stroke                  0\n",
            "Diabetes                0\n",
            "PhysActivity            0\n",
            "Fruits                  0\n",
            "Veggies                 0\n",
            "HvyAlcoholConsump       0\n",
            "AnyHealthcare           0\n",
            "NoDocbcCost             1\n",
            "GenHlth                 1\n",
            "MentHlth                1\n",
            "PhysHlth                1\n",
            "DiffWalk                1\n",
            "Sex                     1\n",
            "Age                     1\n",
            "Education               1\n",
            "Income                  1\n",
            "dtype: int64, 123: HeartDiseaseorAttack    0\n",
            "HighBP                  0\n",
            "HighChol                0\n",
            "CholCheck               0\n",
            "BMI                     0\n",
            "Smoker                  0\n",
            "Stroke                  0\n",
            "Diabetes                0\n",
            "PhysActivity            0\n",
            "Fruits                  0\n",
            "Veggies                 0\n",
            "HvyAlcoholConsump       0\n",
            "AnyHealthcare           0\n",
            "NoDocbcCost             0\n",
            "GenHlth                 0\n",
            "MentHlth                0\n",
            "PhysHlth                0\n",
            "DiffWalk                0\n",
            "Sex                     0\n",
            "Age                     0\n",
            "Education               0\n",
            "Income                  0\n",
            "dtype: int64, 137: RandomForestClassifier(random_state=42), 139: HistGradientBoostingClassifier(), 140: RandomForestClassifier(random_state=42)}, 'get_ipython': <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x7e5af31337f0>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x7e5af3132cb0>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x7e5af3132cb0>, '_': RandomForestClassifier(random_state=42), '__': HistGradientBoostingClassifier(), '___': RandomForestClassifier(random_state=42), '_i': '# Fit the GridSearchCV object\\ngrid_search.fit(X_train, y_train)', '_ii': '# Fit the GridSearchCV object\\ngrid_search.fit(X_train_scaled, y_train)', '_iii': 'X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=df.columns)', '_i1': \"# Define a grid of parameters to search\\nparam_grid = {\\n    'imputer__strategy': ['mean', 'median'],  # Try different strategies for imputation\\n    'classifier__C': [0.1, 1, 10],  # Regularization strength for LogisticRegression\\n    'classifier__solver': ['liblinear', 'saga']  # Optimization algorithms\\n}\", 'param_grid': Ellipsis, '_i2': '# Set up GridSearchCV\\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\\'accuracy\\', verbose=1)\\n\\n# Fit GridSearchCV\\ngrid_search.fit(X_train_scaled, y_train)\\n\\n# Best parameters found\\nprint(\"Best parameters:\", grid_search.best_params_)\\n\\n# Evaluate on the test set\\ny_pred = grid_search.predict(X_test_scaled)\\nprint(classification_report(y_test, y_pred))', '_i3': '# Set up GridSearchCV\\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\\'accuracy\\', verbose=1)\\ngrid_search.df\\n\\n# Fit GridSearchCV\\ngrid_search.fit(X_train_scaled, y_train)\\n\\n# Best parameters found\\nprint(\"Best parameters:\", grid_search.best_params_)\\n\\n# Evaluate on the test set\\ny_pred = grid_search.predict(X_test_scaled)\\nprint(classification_report(y_test, y_pred))', '_i4': 'from sklearn.model_selection import GridSearchCV', 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, '_i5': '# Define the pipeline and parameter grid\\npipeline = ...\\nparam_grid = ...', 'pipeline': Ellipsis, '_i6': \"# Instantiate the GridSearchCV object\\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1)\", 'grid_search': GridSearchCV(cv=5, estimator=Ellipsis, param_grid=Ellipsis, scoring='accuracy',\n",
            "             verbose=1), '_i7': '# Fit the GridSearchCV object\\ngrid_search.fit(X_train_scaled, y_train)', '_i8': '# Fit the GridSearchCV object\\ngrid_search.fit(X_train, y_train)', '_i9': '# Fit the GridSearchCV object\\ngrid_search.fit(X_train_scaled, y_train)\\nX-train_scaled.df', '_i10': '# Fit the GridSearchCV object\\ngrid_search.fit(X_train_scaled, y_train)', '_i11': '# Load the data into X_train_scaled and y_train variables\\n# Replace the following lines with your actual data loading code\\nX_train_scaled = pd.read_csv(\"X_train_scaled.csv\")\\ny_train = pd.read_csv(\"y_train.csv\")', '_i12': 'import pandas as pd', 'pd': <module 'pandas' from '/usr/local/lib/python3.10/dist-packages/pandas/__init__.py'>, '_i13': '# Load the data into X_train_scaled and y_train variables\\n# Replace the following lines with your actual data loading code\\nX_train_scaled = pd.read_csv(\"X_train_scaled.csv\")\\ny_train = pd.read_csv(\"y_train.csv\")', '_i14': '# Load the data into X_train_scaled and y_train variables\\n# Replace the following lines with your actual data loading code\\nX_train_scaled = pd.read_csv(\"heart_disease_health_indicators.csv\")\\ny_train = pd.read_csv(\"heart_disease_health_indicators.csv\")', '_i15': '# Load the data into X_train_scaled and y_train variables\\n# Replace the following lines with your actual data loading code\\nX_train_scaled = pd.read_csv(\"heart_disease_health_indicators.csv\")\\ny_train = pd.read_csv(\"heart_disease_health_indicators.csv\")', '_i16': 'print(X_train_scaled.head())\\nprint(y_train.head())', '_i17': '# Load the data into X_train_scaled and y_train variables\\n# Replace the following lines with your actual data loading code\\nX_train = pd.read_csv(\"heart_disease_health_indicators.csv\")\\ny_train = pd.read_csv(\"heart_disease_health_indicators.csv\")', '_i18': '# Load the data into X_train_scaled and y_train variables\\n# Replace the following lines with your actual data loading code\\nX_train = pd.read_csv(\"/Resources/heart_disease_health_indicators.csv\")\\ny_train = pd.read_csv(\"Resources/heart_disease_health_indicators.csv\")', '_i19': 'print(X_train.head())\\nprint(y_train.head())', '_i20': 'print(X_train.head())\\nprint(y_train.head())', '_i21': \"import numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\", 'np': <module 'numpy' from '/usr/local/lib/python3.10/dist-packages/numpy/__init__.py'>, 'train_test_split': <function train_test_split at 0x7e5ab1f07760>, 'LogisticRegression': <class 'sklearn.linear_model._logistic.LogisticRegression'>, 'StandardScaler': <class 'sklearn.preprocessing._data.StandardScaler'>, 'accuracy_score': <function accuracy_score at 0x7e5ab1fa9480>, 'confusion_matrix': <function confusion_matrix at 0x7e5ab1fa9360>, 'classification_report': <function classification_report at 0x7e5ab1fa9d80>, 'plt': <module 'matplotlib.pyplot' from '/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py'>, 'sns': <module 'seaborn' from '/usr/local/lib/python3.10/dist-packages/seaborn/__init__.py'>, 'drive': <module 'google.colab.drive' from '/usr/local/lib/python3.10/dist-packages/google/colab/drive.py'>, '_i22': \"# Assuming you've uploaded the CSV to your Google Colab session\\nheart_df = pd.read_csv('heart_disease_health_indicators.csv')\", '_i23': '#Identify missing values\\nmissing_values = heart_df.info()', '_i24': \"# Assuming you've uploaded the CSV to your Google Colab session\\ndf = pd.read_csv('/content/heart_disease_health_indicators.csv')\", '_i25': \"# Assuming you've uploaded the CSV to your Google Colab session\\ndf = pd.read_csv('/content/heart_disease_health_indicators.csv')\", 'df':         HeartDiseaseorAttack  HighBP  HighChol  CholCheck  BMI  Smoker  \\\n",
            "0                          0       1         1          1   40       1   \n",
            "1                          0       0         0          0   25       1   \n",
            "2                          0       1         1          1   28       0   \n",
            "3                          0       1         0          1   27       0   \n",
            "4                          0       1         1          1   24       0   \n",
            "...                      ...     ...       ...        ...  ...     ...   \n",
            "137858                     1       1         0          1   53       0   \n",
            "137859                     0       0         0          1   36       0   \n",
            "137860                     0       0         0          0   19       1   \n",
            "137861                     1       1         1          1   30       0   \n",
            "137862                     0       1         1          1   38       0   \n",
            "\n",
            "        Stroke  Diabetes  PhysActivity  Fruits  ...  AnyHealthcare  \\\n",
            "0            0         0             0       0  ...              1   \n",
            "1            0         0             1       0  ...              0   \n",
            "2            0         0             0       1  ...              1   \n",
            "3            0         0             1       1  ...              1   \n",
            "4            0         0             1       1  ...              1   \n",
            "...        ...       ...           ...     ...  ...            ...   \n",
            "137858       0         0             1       0  ...              1   \n",
            "137859       0         0             1       0  ...              1   \n",
            "137860       0         0             1       0  ...              0   \n",
            "137861       0         2             1       0  ...              1   \n",
            "137862       0         2             1       0  ...              1   \n",
            "\n",
            "        NoDocbcCost  GenHlth  MentHlth  PhysHlth  DiffWalk  Sex   Age  \\\n",
            "0               0.0      5.0      18.0      15.0       1.0  0.0   9.0   \n",
            "1               1.0      3.0       0.0       0.0       0.0  0.0   7.0   \n",
            "2               1.0      5.0      30.0      30.0       1.0  0.0   9.0   \n",
            "3               0.0      2.0       0.0       0.0       0.0  0.0  11.0   \n",
            "4               0.0      2.0       3.0       0.0       0.0  0.0  11.0   \n",
            "...             ...      ...       ...       ...       ...  ...   ...   \n",
            "137858          0.0      3.0       2.0       1.0       0.0  1.0   2.0   \n",
            "137859          0.0      1.0       0.0       0.0       0.0  0.0   3.0   \n",
            "137860          0.0      2.0       0.0       0.0       0.0  1.0   5.0   \n",
            "137861          0.0      5.0       0.0       0.0       1.0  0.0  13.0   \n",
            "137862          NaN      NaN       NaN       NaN       NaN  NaN   NaN   \n",
            "\n",
            "        Education  Income  \n",
            "0             4.0     3.0  \n",
            "1             6.0     1.0  \n",
            "2             4.0     8.0  \n",
            "3             3.0     6.0  \n",
            "4             5.0     4.0  \n",
            "...           ...     ...  \n",
            "137858        5.0     2.0  \n",
            "137859        5.0     3.0  \n",
            "137860        4.0     6.0  \n",
            "137861        4.0     2.0  \n",
            "137862        NaN     NaN  \n",
            "\n",
            "[137863 rows x 22 columns], '_i26': '#Identify missing values\\nmissing_values = heart_df.info()', '_i27': '#Removing the null values\\n\\ndf.isnull().sum()', '_27': HeartDiseaseorAttack    0\n",
            "HighBP                  0\n",
            "HighChol                0\n",
            "CholCheck               0\n",
            "BMI                     0\n",
            "Smoker                  0\n",
            "Stroke                  0\n",
            "Diabetes                0\n",
            "PhysActivity            0\n",
            "Fruits                  0\n",
            "Veggies                 0\n",
            "HvyAlcoholConsump       0\n",
            "AnyHealthcare           0\n",
            "NoDocbcCost             1\n",
            "GenHlth                 1\n",
            "MentHlth                1\n",
            "PhysHlth                1\n",
            "DiffWalk                1\n",
            "Sex                     1\n",
            "Age                     1\n",
            "Education               1\n",
            "Income                  1\n",
            "dtype: int64, '_i28': \"# Assuming you've uploaded the CSV to your Google Colab session\\nheart_df = pd.read_csv('/content/heart_disease_health_indicators.csv')\", 'heart_df':         HeartDiseaseorAttack  HighBP  HighChol  CholCheck  BMI  Smoker  \\\n",
            "0                          0       1         1          1   40       1   \n",
            "1                          0       0         0          0   25       1   \n",
            "2                          0       1         1          1   28       0   \n",
            "3                          0       1         0          1   27       0   \n",
            "4                          0       1         1          1   24       0   \n",
            "...                      ...     ...       ...        ...  ...     ...   \n",
            "253656                     0       0         0          1   25       0   \n",
            "253657                     0       0         1          1   24       0   \n",
            "253658                     0       0         0          0   27       0   \n",
            "253659                     0       0         1          1   37       0   \n",
            "253660                     0       0         1          1   34       1   \n",
            "\n",
            "        Stroke  Diabetes  PhysActivity  Fruits  ...  AnyHealthcare  \\\n",
            "0            0         0             0       0  ...              1   \n",
            "1            0         0             1       0  ...              0   \n",
            "2            0         0             0       1  ...              1   \n",
            "3            0         0             1       1  ...              1   \n",
            "4            0         0             1       1  ...              1   \n",
            "...        ...       ...           ...     ...  ...            ...   \n",
            "253656       0         0             1       1  ...              1   \n",
            "253657       0         0             0       0  ...              1   \n",
            "253658       0         0             1       0  ...              1   \n",
            "253659       0         2             0       0  ...              1   \n",
            "253660       0         0             0       1  ...              1   \n",
            "\n",
            "        NoDocbcCost  GenHlth  MentHlth  PhysHlth  DiffWalk  Sex  Age  \\\n",
            "0                 0        5        18        15         1    0    9   \n",
            "1                 1        3         0         0         0    0    7   \n",
            "2                 1        5        30        30         1    0    9   \n",
            "3                 0        2         0         0         0    0   11   \n",
            "4                 0        2         3         0         0    0   11   \n",
            "...             ...      ...       ...       ...       ...  ...  ...   \n",
            "253656            0        1         0         0         0    0    4   \n",
            "253657            0        3         0         0         0    0    7   \n",
            "253658            1        2         0         0         0    0    3   \n",
            "253659            0        4         0         0         0    0    6   \n",
            "253660            0        3         0         2         1    0    7   \n",
            "\n",
            "        Education  Income  \n",
            "0               4       3  \n",
            "1               6       1  \n",
            "2               4       8  \n",
            "3               3       6  \n",
            "4               5       4  \n",
            "...           ...     ...  \n",
            "253656          6       8  \n",
            "253657          5       3  \n",
            "253658          6       5  \n",
            "253659          4       1  \n",
            "253660          4       3  \n",
            "\n",
            "[253661 rows x 22 columns], '_i29': '#Identify missing values\\nmissing_values = heart_df.info()', 'missing_values': None, '_i30': '#Removing the null values\\n\\ndf.isnull().sum()', '_30': HeartDiseaseorAttack    0\n",
            "HighBP                  0\n",
            "HighChol                0\n",
            "CholCheck               0\n",
            "BMI                     0\n",
            "Smoker                  0\n",
            "Stroke                  0\n",
            "Diabetes                0\n",
            "PhysActivity            0\n",
            "Fruits                  0\n",
            "Veggies                 0\n",
            "HvyAlcoholConsump       0\n",
            "AnyHealthcare           0\n",
            "NoDocbcCost             1\n",
            "GenHlth                 1\n",
            "MentHlth                1\n",
            "PhysHlth                1\n",
            "DiffWalk                1\n",
            "Sex                     1\n",
            "Age                     1\n",
            "Education               1\n",
            "Income                  1\n",
            "dtype: int64, '_i31': \"columns_with_missing_values = ['Diabetes', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income']\\ndf_cleaned_specific = df.dropna(subset=columns_with_missing_values)\", 'columns_with_missing_values': ['Diabetes', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income'], 'df_cleaned_specific':         HeartDiseaseorAttack  HighBP  HighChol  CholCheck  BMI  Smoker  \\\n",
            "0                          0       1         1          1   40       1   \n",
            "1                          0       0         0          0   25       1   \n",
            "2                          0       1         1          1   28       0   \n",
            "3                          0       1         0          1   27       0   \n",
            "4                          0       1         1          1   24       0   \n",
            "...                      ...     ...       ...        ...  ...     ...   \n",
            "137857                     0       0         0          1   32       0   \n",
            "137858                     1       1         0          1   53       0   \n",
            "137859                     0       0         0          1   36       0   \n",
            "137860                     0       0         0          0   19       1   \n",
            "137861                     1       1         1          1   30       0   \n",
            "\n",
            "        Stroke  Diabetes  PhysActivity  Fruits  ...  AnyHealthcare  \\\n",
            "0            0         0             0       0  ...              1   \n",
            "1            0         0             1       0  ...              0   \n",
            "2            0         0             0       1  ...              1   \n",
            "3            0         0             1       1  ...              1   \n",
            "4            0         0             1       1  ...              1   \n",
            "...        ...       ...           ...     ...  ...            ...   \n",
            "137857       0         0             1       1  ...              0   \n",
            "137858       0         0             1       0  ...              1   \n",
            "137859       0         0             1       0  ...              1   \n",
            "137860       0         0             1       0  ...              0   \n",
            "137861       0         2             1       0  ...              1   \n",
            "\n",
            "        NoDocbcCost  GenHlth  MentHlth  PhysHlth  DiffWalk  Sex   Age  \\\n",
            "0               0.0      5.0      18.0      15.0       1.0  0.0   9.0   \n",
            "1               1.0      3.0       0.0       0.0       0.0  0.0   7.0   \n",
            "2               1.0      5.0      30.0      30.0       1.0  0.0   9.0   \n",
            "3               0.0      2.0       0.0       0.0       0.0  0.0  11.0   \n",
            "4               0.0      2.0       3.0       0.0       0.0  0.0  11.0   \n",
            "...             ...      ...       ...       ...       ...  ...   ...   \n",
            "137857          0.0      1.0       0.0       0.0       0.0  0.0   7.0   \n",
            "137858          0.0      3.0       2.0       1.0       0.0  1.0   2.0   \n",
            "137859          0.0      1.0       0.0       0.0       0.0  0.0   3.0   \n",
            "137860          0.0      2.0       0.0       0.0       0.0  1.0   5.0   \n",
            "137861          0.0      5.0       0.0       0.0       1.0  0.0  13.0   \n",
            "\n",
            "        Education  Income  \n",
            "0             4.0     3.0  \n",
            "1             6.0     1.0  \n",
            "2             4.0     8.0  \n",
            "3             3.0     6.0  \n",
            "4             5.0     4.0  \n",
            "...           ...     ...  \n",
            "137857        4.0     6.0  \n",
            "137858        5.0     2.0  \n",
            "137859        5.0     3.0  \n",
            "137860        4.0     6.0  \n",
            "137861        4.0     2.0  \n",
            "\n",
            "[137862 rows x 22 columns], '_i32': \"import numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\", '_i33': \"# Assuming you've uploaded the CSV to your Google Colab session\\nheart_df = pd.read_csv('/content/heart_disease_health_indicators.csv')\", '_i34': '#Identify missing values\\nmissing_values = heart_df.info()', '_i35': '#Removing the null values\\n\\ndf.isnull().sum()', '_35': HeartDiseaseorAttack    0\n",
            "HighBP                  0\n",
            "HighChol                0\n",
            "CholCheck               0\n",
            "BMI                     0\n",
            "Smoker                  0\n",
            "Stroke                  0\n",
            "Diabetes                0\n",
            "PhysActivity            0\n",
            "Fruits                  0\n",
            "Veggies                 0\n",
            "HvyAlcoholConsump       0\n",
            "AnyHealthcare           0\n",
            "NoDocbcCost             1\n",
            "GenHlth                 1\n",
            "MentHlth                1\n",
            "PhysHlth                1\n",
            "DiffWalk                1\n",
            "Sex                     1\n",
            "Age                     1\n",
            "Education               1\n",
            "Income                  1\n",
            "dtype: int64, '_i36': \"columns_with_missing_values = ['Diabetes', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income']\\ndf_cleaned_specific = df.dropna(subset=columns_with_missing_values)\", '_i37': '#df.isnull().sum()', '_i38': \"# Assuming there are no categorical variables to encode in this dataset\\n# Select features and target\\nX = df.drop('HeartDiseaseorAttack', axis=1)  # Features\\ny = df['HeartDiseaseorAttack']  # Target variable\", 'X':         HighBP  HighChol  CholCheck  BMI  Smoker  Stroke  Diabetes  \\\n",
            "0            1         1          1   40       1       0         0   \n",
            "1            0         0          0   25       1       0         0   \n",
            "2            1         1          1   28       0       0         0   \n",
            "3            1         0          1   27       0       0         0   \n",
            "4            1         1          1   24       0       0         0   \n",
            "...        ...       ...        ...  ...     ...     ...       ...   \n",
            "137858       1         0          1   53       0       0         0   \n",
            "137859       0         0          1   36       0       0         0   \n",
            "137860       0         0          0   19       1       0         0   \n",
            "137861       1         1          1   30       0       0         2   \n",
            "137862       1         1          1   38       0       0         2   \n",
            "\n",
            "        PhysActivity  Fruits  Veggies  ...  AnyHealthcare  NoDocbcCost  \\\n",
            "0                  0       0        1  ...              1          0.0   \n",
            "1                  1       0        0  ...              0          1.0   \n",
            "2                  0       1        0  ...              1          1.0   \n",
            "3                  1       1        1  ...              1          0.0   \n",
            "4                  1       1        1  ...              1          0.0   \n",
            "...              ...     ...      ...  ...            ...          ...   \n",
            "137858             1       0        1  ...              1          0.0   \n",
            "137859             1       0        1  ...              1          0.0   \n",
            "137860             1       0        1  ...              0          0.0   \n",
            "137861             1       0        1  ...              1          0.0   \n",
            "137862             1       0        0  ...              1          NaN   \n",
            "\n",
            "        GenHlth  MentHlth  PhysHlth  DiffWalk  Sex   Age  Education  Income  \n",
            "0           5.0      18.0      15.0       1.0  0.0   9.0        4.0     3.0  \n",
            "1           3.0       0.0       0.0       0.0  0.0   7.0        6.0     1.0  \n",
            "2           5.0      30.0      30.0       1.0  0.0   9.0        4.0     8.0  \n",
            "3           2.0       0.0       0.0       0.0  0.0  11.0        3.0     6.0  \n",
            "4           2.0       3.0       0.0       0.0  0.0  11.0        5.0     4.0  \n",
            "...         ...       ...       ...       ...  ...   ...        ...     ...  \n",
            "137858      3.0       2.0       1.0       0.0  1.0   2.0        5.0     2.0  \n",
            "137859      1.0       0.0       0.0       0.0  0.0   3.0        5.0     3.0  \n",
            "137860      2.0       0.0       0.0       0.0  1.0   5.0        4.0     6.0  \n",
            "137861      5.0       0.0       0.0       1.0  0.0  13.0        4.0     2.0  \n",
            "137862      NaN       NaN       NaN       NaN  NaN   NaN        NaN     NaN  \n",
            "\n",
            "[137863 rows x 21 columns], 'y': 0         0\n",
            "1         0\n",
            "2         0\n",
            "3         0\n",
            "4         0\n",
            "         ..\n",
            "137858    1\n",
            "137859    0\n",
            "137860    0\n",
            "137861    1\n",
            "137862    0\n",
            "Name: HeartDiseaseorAttack, Length: 137863, dtype: int64, '_i39': 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)', 'X_train':              0         1         2         3         4         5         6   \\\n",
            "0       1.15323 -0.861527  0.196519  0.223245 -0.896897 -0.207032 -0.425294   \n",
            "1       1.15323 -0.861527  0.196519  0.798741 -0.896897 -0.207032 -0.425294   \n",
            "2      -0.86713 -0.861527  0.196519 -0.783872 -0.896897 -0.207032 -0.425294   \n",
            "3      -0.86713 -0.861527  0.196519 -1.503241 -0.896897 -0.207032 -0.425294   \n",
            "4      -0.86713 -0.861527 -5.088560  0.510993  1.114955 -0.207032 -0.425294   \n",
            "...         ...       ...       ...       ...       ...       ...       ...   \n",
            "110285  1.15323  1.160730  0.196519  0.223245 -0.896897  4.830162  2.442677   \n",
            "110286  1.15323 -0.861527  0.196519  2.381353  1.114955 -0.207032 -0.425294   \n",
            "110287  1.15323  1.160730  0.196519  0.079371  1.114955 -0.207032 -0.425294   \n",
            "110288 -0.86713  1.160730  0.196519 -0.496124 -0.896897 -0.207032 -0.425294   \n",
            "110289  1.15323 -0.861527  0.196519  0.942614 -0.896897 -0.207032  2.442677   \n",
            "\n",
            "              7         8         9   ...        11        12        13  \\\n",
            "0      -1.780336  0.753199  0.484249  ...  0.225525 -0.295715  0.466729   \n",
            "1      -1.780336  0.753199  0.484249  ...  0.225525 -0.295715  0.466729   \n",
            "2      -1.780336  0.753199 -2.065052  ...  0.225525 -0.295715  1.404969   \n",
            "3       0.561692  0.753199  0.484249  ...  0.225525 -0.295715  0.466729   \n",
            "4       0.561692  0.753199  0.484249  ...  0.225525 -0.295715 -0.471510   \n",
            "...          ...       ...       ...  ...       ...       ...       ...   \n",
            "110285  0.561692  0.753199  0.484249  ...  0.225525 -0.295715  1.404969   \n",
            "110286  0.561692  0.753199 -2.065052  ...  0.225525  3.381634 -1.409750   \n",
            "110287  0.561692 -1.327670  0.484249  ...  0.225525 -0.295715  0.466729   \n",
            "110288  0.561692  0.753199  0.484249  ...  0.225525 -0.295715 -0.471510   \n",
            "110289 -1.780336  0.753199  0.484249  ...  0.225525 -0.295715  0.466729   \n",
            "\n",
            "              14        15        16        17        18       19        20  \n",
            "0      -0.425506 -0.483517 -0.445100 -0.884733  0.631794 -1.09985  0.928495  \n",
            "1      -0.425506  2.985784 -0.445100  1.130284 -0.351821 -0.07421  0.928495  \n",
            "2      -0.425506  1.829351  2.246685 -0.884733  0.959666 -1.09985  0.440835  \n",
            "3      -0.013376 -0.483517  2.246685 -0.884733 -0.023949  0.95143  0.440835  \n",
            "4      -0.150753 -0.483517 -0.445100 -0.884733 -1.663307 -0.07421  0.928495  \n",
            "...          ...       ...       ...       ...       ...      ...       ...  \n",
            "110285 -0.425506 -0.252230  2.246685 -0.884733  1.287537 -0.07421 -0.534485  \n",
            "110286  1.635143 -0.483517 -0.445100 -0.884733 -1.007564 -1.09985 -2.485125  \n",
            "110287 -0.425506 -0.483517 -0.445100  1.130284  0.959666  0.95143  0.440835  \n",
            "110288 -0.425506 -0.136587 -0.445100 -0.884733  0.303923  0.95143  0.928495  \n",
            "110289  1.635143 -0.483517  2.246685 -0.884733  0.959666 -0.07421 -0.046825  \n",
            "\n",
            "[110290 rows x 21 columns], 'X_test': array([[ 1.15322988, -0.86152654,  0.19651926, ...,  0.63179421,\n",
            "         0.95143027, -0.53448492],\n",
            "       [-0.8671298 , -0.86152654,  0.19651926, ..., -0.67969218,\n",
            "        -0.07420989,  0.44083509],\n",
            "       [-0.8671298 , -0.86152654,  0.19651926, ...,  0.30392261,\n",
            "         0.95143027, -0.04682491],\n",
            "       ...,\n",
            "       [-0.8671298 ,  1.16073035,  0.19651926, ...,  0.30392261,\n",
            "        -1.09985004, -0.53448492],\n",
            "       [-0.8671298 ,  1.16073035,  0.19651926, ...,  0.30392261,\n",
            "        -0.07420989,  0.9284951 ],\n",
            "       [-0.8671298 , -0.86152654,  0.19651926, ..., -0.67969218,\n",
            "         0.95143027,  0.9284951 ]]), 'y_train':           0\n",
            "0       0.0\n",
            "1       0.0\n",
            "2       0.0\n",
            "3       0.0\n",
            "4       0.0\n",
            "...     ...\n",
            "110285  0.0\n",
            "110286  0.0\n",
            "110287  0.0\n",
            "110288  0.0\n",
            "110289  0.0\n",
            "\n",
            "[110290 rows x 1 columns], 'y_test': 124731    0\n",
            "62762     0\n",
            "111642    0\n",
            "113117    0\n",
            "31907     0\n",
            "         ..\n",
            "17562     0\n",
            "95059     1\n",
            "90559     0\n",
            "129007    0\n",
            "38758     0\n",
            "Name: HeartDiseaseorAttack, Length: 27573, dtype: int64, '_i40': 'scaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)', 'scaler': StandardScaler(), '_i41': 'pip install scikit-learn', '_exit_code': 0, '_i42': 'from sklearn.linear_model import LogisticRegression', '_i43': 'model = LogisticRegression(random_state=42)', 'model': RandomForestClassifier(random_state=42), '_i44': 'from sklearn.ensemble import RandomForestClassifier\\n\\n# Example of initializing a RandomForestClassifier, which does use n_estimators\\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)', 'RandomForestClassifier': <class 'sklearn.ensemble._forest.RandomForestClassifier'>, '_i45': 'print(y_train.head())', '_i46': 'print(y_train.isna().sum())', '_i47': '#If y_train contains missing values:\\ny_train = y_train.dropna()', '_i48': '!pip install scikit-learn\\nfrom sklearn.impute import SimpleImputer', 'SimpleImputer': <class 'sklearn.impute._base.SimpleImputer'>, '_i49': 'imputer = SimpleImputer(strategy=\"mean\")\\ny_train = imputer.fit_transform(y_train.values.reshape(-1, 1))', 'imputer': SimpleImputer(), '_i50': \"from sklearn.impute import SimpleImputer\\nimport numpy as np\\n\\n# Create an imputer object with a strategy of filling missing values with the mean of the column\\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\\n\\n# Fit on the training data and transform it\\nX_train_imputed = imputer.fit_transform(X_train)\\n\\n# Don't forget to transform the test data as well, using the same imputer (without fitting it again)\\nX_test_imputed = imputer.transform(X_test)\\n\\n# Now you can fit the model with the imputed data\\nmodel.fit(X_train_imputed, y_train)\", 'X_train_imputed': array([[ 1.15322988, -0.86152654,  0.19651926, ...,  0.63179421,\n",
            "        -1.09985004,  0.9284951 ],\n",
            "       [ 1.15322988, -0.86152654,  0.19651926, ..., -0.35182058,\n",
            "        -0.07420989,  0.9284951 ],\n",
            "       [-0.8671298 , -0.86152654,  0.19651926, ...,  0.9596658 ,\n",
            "        -1.09985004,  0.44083509],\n",
            "       ...,\n",
            "       [ 1.15322988,  1.16073035,  0.19651926, ...,  0.9596658 ,\n",
            "         0.95143027,  0.44083509],\n",
            "       [-0.8671298 ,  1.16073035,  0.19651926, ...,  0.30392261,\n",
            "         0.95143027,  0.9284951 ],\n",
            "       [ 1.15322988, -0.86152654,  0.19651926, ...,  0.9596658 ,\n",
            "        -0.07420989, -0.04682491]]), 'X_test_imputed': array([[ 1.15322988, -0.86152654,  0.19651926, ...,  0.63179421,\n",
            "         0.95143027, -0.53448492],\n",
            "       [-0.8671298 , -0.86152654,  0.19651926, ..., -0.67969218,\n",
            "        -0.07420989,  0.44083509],\n",
            "       [-0.8671298 , -0.86152654,  0.19651926, ...,  0.30392261,\n",
            "         0.95143027, -0.04682491],\n",
            "       ...,\n",
            "       [-0.8671298 ,  1.16073035,  0.19651926, ...,  0.30392261,\n",
            "        -1.09985004, -0.53448492],\n",
            "       [-0.8671298 ,  1.16073035,  0.19651926, ...,  0.30392261,\n",
            "        -0.07420989,  0.9284951 ],\n",
            "       [-0.8671298 , -0.86152654,  0.19651926, ..., -0.67969218,\n",
            "         0.95143027,  0.9284951 ]]), '_50': RandomForestClassifier(random_state=42), '_i51': '# Drop rows with any missing values\\ndf_cleaned = df.dropna()\\n\\n# After cleaning, proceed with your feature/target selection, data splitting, and so on', 'df_cleaned':         HeartDiseaseorAttack  HighBP  HighChol  CholCheck  BMI  Smoker  \\\n",
            "0                          0       1         1          1   40       1   \n",
            "1                          0       0         0          0   25       1   \n",
            "2                          0       1         1          1   28       0   \n",
            "3                          0       1         0          1   27       0   \n",
            "4                          0       1         1          1   24       0   \n",
            "...                      ...     ...       ...        ...  ...     ...   \n",
            "137857                     0       0         0          1   32       0   \n",
            "137858                     1       1         0          1   53       0   \n",
            "137859                     0       0         0          1   36       0   \n",
            "137860                     0       0         0          0   19       1   \n",
            "137861                     1       1         1          1   30       0   \n",
            "\n",
            "        Stroke  Diabetes  PhysActivity  Fruits  ...  AnyHealthcare  \\\n",
            "0            0         0             0       0  ...              1   \n",
            "1            0         0             1       0  ...              0   \n",
            "2            0         0             0       1  ...              1   \n",
            "3            0         0             1       1  ...              1   \n",
            "4            0         0             1       1  ...              1   \n",
            "...        ...       ...           ...     ...  ...            ...   \n",
            "137857       0         0             1       1  ...              0   \n",
            "137858       0         0             1       0  ...              1   \n",
            "137859       0         0             1       0  ...              1   \n",
            "137860       0         0             1       0  ...              0   \n",
            "137861       0         2             1       0  ...              1   \n",
            "\n",
            "        NoDocbcCost  GenHlth  MentHlth  PhysHlth  DiffWalk  Sex   Age  \\\n",
            "0               0.0      5.0      18.0      15.0       1.0  0.0   9.0   \n",
            "1               1.0      3.0       0.0       0.0       0.0  0.0   7.0   \n",
            "2               1.0      5.0      30.0      30.0       1.0  0.0   9.0   \n",
            "3               0.0      2.0       0.0       0.0       0.0  0.0  11.0   \n",
            "4               0.0      2.0       3.0       0.0       0.0  0.0  11.0   \n",
            "...             ...      ...       ...       ...       ...  ...   ...   \n",
            "137857          0.0      1.0       0.0       0.0       0.0  0.0   7.0   \n",
            "137858          0.0      3.0       2.0       1.0       0.0  1.0   2.0   \n",
            "137859          0.0      1.0       0.0       0.0       0.0  0.0   3.0   \n",
            "137860          0.0      2.0       0.0       0.0       0.0  1.0   5.0   \n",
            "137861          0.0      5.0       0.0       0.0       1.0  0.0  13.0   \n",
            "\n",
            "        Education  Income  \n",
            "0             4.0     3.0  \n",
            "1             6.0     1.0  \n",
            "2             4.0     8.0  \n",
            "3             3.0     6.0  \n",
            "4             5.0     4.0  \n",
            "...           ...     ...  \n",
            "137857        4.0     6.0  \n",
            "137858        5.0     2.0  \n",
            "137859        5.0     3.0  \n",
            "137860        4.0     6.0  \n",
            "137861        4.0     2.0  \n",
            "\n",
            "[137862 rows x 22 columns], '_i52': 'from sklearn.experimental import enable_hist_gradient_boosting  # noqa\\nfrom sklearn.ensemble import HistGradientBoostingClassifier\\n\\n# Initialize the model that can handle NaN values\\nmodel_with_nan_support = HistGradientBoostingClassifier()\\n\\n# This model can be directly fitted with data containing NaNs\\nmodel_with_nan_support.fit(X_train, y_train)', 'enable_hist_gradient_boosting': <module 'sklearn.experimental.enable_hist_gradient_boosting' from '/usr/local/lib/python3.10/dist-packages/sklearn/experimental/enable_hist_gradient_boosting.py'>, 'HistGradientBoostingClassifier': <class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>, 'model_with_nan_support': HistGradientBoostingClassifier(), '_52': HistGradientBoostingClassifier(), '_i53': 'model.fit?', '_i54': 'model.fit(X_train, y_train)', '_54': RandomForestClassifier(random_state=42), '_i55': 'y_pred = model.predict(X_test)', '_i56': \"# Assuming you've uploaded the CSV to your Google Colab session\\nheart_df = pd.read_csv('heart_disease_health_indicators.csv')\", '_i57': '#Identify missing values\\nmissing_values = heart_df.info()', '_i58': '#Removing the null values\\n\\ndf.isnull().sum()', '_58': HeartDiseaseorAttack    0\n",
            "HighBP                  0\n",
            "HighChol                0\n",
            "CholCheck               0\n",
            "BMI                     0\n",
            "Smoker                  0\n",
            "Stroke                  0\n",
            "Diabetes                0\n",
            "PhysActivity            0\n",
            "Fruits                  0\n",
            "Veggies                 0\n",
            "HvyAlcoholConsump       0\n",
            "AnyHealthcare           0\n",
            "NoDocbcCost             1\n",
            "GenHlth                 1\n",
            "MentHlth                1\n",
            "PhysHlth                1\n",
            "DiffWalk                1\n",
            "Sex                     1\n",
            "Age                     1\n",
            "Education               1\n",
            "Income                  1\n",
            "dtype: int64, '_i59': \"import numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\", '_i60': \"# Assuming you've uploaded the CSV to your Google Colab session\\nheart_df = pd.read_csv('heart_disease_health_indicators.csv')\", '_i61': '#Identify missing values\\nmissing_values = heart_df.info()', '_i62': '#Removing the null values\\n\\ndf.isnull().sum()', '_62': HeartDiseaseorAttack    0\n",
            "HighBP                  0\n",
            "HighChol                0\n",
            "CholCheck               0\n",
            "BMI                     0\n",
            "Smoker                  0\n",
            "Stroke                  0\n",
            "Diabetes                0\n",
            "PhysActivity            0\n",
            "Fruits                  0\n",
            "Veggies                 0\n",
            "HvyAlcoholConsump       0\n",
            "AnyHealthcare           0\n",
            "NoDocbcCost             1\n",
            "GenHlth                 1\n",
            "MentHlth                1\n",
            "PhysHlth                1\n",
            "DiffWalk                1\n",
            "Sex                     1\n",
            "Age                     1\n",
            "Education               1\n",
            "Income                  1\n",
            "dtype: int64, '_i63': \"columns_with_missing_values = ['Diabetes', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income']\\ndf_cleaned_specific = df.dropna(subset=columns_with_missing_values)\", '_i64': '#df.isnull().sum()', '_i65': \"# Assuming there are no categorical variables to encode in this dataset\\n# Select features and target\\nX = df.drop('HeartDiseaseorAttack', axis=1)  # Features\\ny = df['HeartDiseaseorAttack']  # Target variable\", '_i66': 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)', '_i67': 'scaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)', '_i68': 'pip install scikit-learn', '_i69': 'from sklearn.linear_model import LogisticRegression', '_i70': 'model = LogisticRegression(random_state=42)', '_i71': 'from sklearn.ensemble import RandomForestClassifier\\n\\n# Example of initializing a RandomForestClassifier, which does use n_estimators\\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)', '_i72': 'print(y_train.head())', '_i73': 'print(y_train.isna().sum())', '_i74': '#If y_train contains missing values:\\ny_train = y_train.dropna()', '_i75': '!pip install scikit-learn\\nfrom sklearn.impute import SimpleImputer', '_i76': 'imputer = SimpleImputer(strategy=\"mean\")\\ny_train = imputer.fit_transform(y_train.values.reshape(-1, 1))', '_i77': \"from sklearn.impute import SimpleImputer\\nimport numpy as np\\n\\n# Create an imputer object with a strategy of filling missing values with the mean of the column\\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\\n\\n# Fit on the training data and transform it\\nX_train_imputed = imputer.fit_transform(X_train)\\n\\n# Don't forget to transform the test data as well, using the same imputer (without fitting it again)\\nX_test_imputed = imputer.transform(X_test)\\n\\n# Now you can fit the model with the imputed data\\nmodel.fit(X_train_imputed, y_train)\", '_77': RandomForestClassifier(random_state=42), '_i78': '# Drop rows with any missing values\\ndf_cleaned = df.dropna()\\n\\n# After cleaning, proceed with your feature/target selection, data splitting, and so on', '_i79': 'from sklearn.experimental import enable_hist_gradient_boosting  # noqa\\nfrom sklearn.ensemble import HistGradientBoostingClassifier\\n\\n# Initialize the model that can handle NaN values\\nmodel_with_nan_support = HistGradientBoostingClassifier()\\n\\n# This model can be directly fitted with data containing NaNs\\nmodel_with_nan_support.fit(X_train, y_train)', '_79': HistGradientBoostingClassifier(), '_i80': 'model.fit?', '_i81': 'model.fit(X_train, y_train)', '_81': RandomForestClassifier(random_state=42), '_i82': 'y_pred = model.predict(X_test)', '_i83': \"import numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\", 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, '_i84': \"import numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\", '_i85': \"# Assuming you've uploaded the CSV to your Google Colab session\\nheart_df = pd.read_csv('heart_disease_health_indicators.csv')\", '_i86': '#Identify missing values\\nmissing_values = heart_df.info()', '_i87': '#Removing the null values\\n\\ndf.isnull().sum()', '_87': HeartDiseaseorAttack    0\n",
            "HighBP                  0\n",
            "HighChol                0\n",
            "CholCheck               0\n",
            "BMI                     0\n",
            "Smoker                  0\n",
            "Stroke                  0\n",
            "Diabetes                0\n",
            "PhysActivity            0\n",
            "Fruits                  0\n",
            "Veggies                 0\n",
            "HvyAlcoholConsump       0\n",
            "AnyHealthcare           0\n",
            "NoDocbcCost             1\n",
            "GenHlth                 1\n",
            "MentHlth                1\n",
            "PhysHlth                1\n",
            "DiffWalk                1\n",
            "Sex                     1\n",
            "Age                     1\n",
            "Education               1\n",
            "Income                  1\n",
            "dtype: int64, '_i88': \"columns_with_missing_values = ['Diabetes', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income']\\ndf_cleaned_specific = df.dropna(subset=columns_with_missing_values)\", '_i89': '#df.isnull().sum()', '_i90': \"# Assuming there are no categorical variables to encode in this dataset\\n# Select features and target\\nX = df.drop('HeartDiseaseorAttack', axis=1)  # Features\\ny = df['HeartDiseaseorAttack']  # Target variable\", '_i91': 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)', '_i92': 'scaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)', '_i93': 'pip install scikit-learn', '_i94': 'from sklearn.linear_model import LogisticRegression', '_i95': 'model = LogisticRegression(random_state=42)', '_i96': 'from sklearn.ensemble import RandomForestClassifier\\n\\n# Example of initializing a RandomForestClassifier, which does use n_estimators\\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)', '_i97': 'print(y_train.head())', '_i98': 'print(y_train.isna().sum())', '_i99': '#If y_train contains missing values:\\ny_train = y_train.dropna()', '_i100': '!pip install scikit-learn\\nfrom sklearn.impute import SimpleImputer', '_i101': 'imputer = SimpleImputer(strategy=\"mean\")\\ny_train = imputer.fit_transform(y_train.values.reshape(-1, 1))', '_i102': \"from sklearn.impute import SimpleImputer\\nimport numpy as np\\n\\n# Create an imputer object with a strategy of filling missing values with the mean of the column\\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\\n\\n# Fit on the training data and transform it\\nX_train_imputed = imputer.fit_transform(X_train)\\n\\n# Don't forget to transform the test data as well, using the same imputer (without fitting it again)\\nX_test_imputed = imputer.transform(X_test)\\n\\n# Now you can fit the model with the imputed data\\nmodel.fit(X_train_imputed, y_train)\", '_102': RandomForestClassifier(random_state=42), '_i103': '# Drop rows with any missing values\\ndf_cleaned = df.dropna()\\n\\n# After cleaning, proceed with your feature/target selection, data splitting, and so on', '_i104': 'from sklearn.experimental import enable_hist_gradient_boosting  # noqa\\nfrom sklearn.ensemble import HistGradientBoostingClassifier\\n\\n# Initialize the model that can handle NaN values\\nmodel_with_nan_support = HistGradientBoostingClassifier()\\n\\n# This model can be directly fitted with data containing NaNs\\nmodel_with_nan_support.fit(X_train, y_train)', '_104': HistGradientBoostingClassifier(), '_i105': 'model.fit?', '_i106': 'model.fit(X_train, y_train)', '_106': RandomForestClassifier(random_state=42), '_i107': 'y_pred = model.predict(X_test)', '_i108': \"# Define a pipeline\\npipeline = Pipeline([\\n    ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values\\n    ('scaler', StandardScaler()),  # Scale features\\n    ('classifier', LogisticRegression(random_state=42))  # Classifier\\n])\\n\\n# Define a grid of parameters to search\\nparam_grid = {\\n    'imputer__strategy': ['mean', 'median'],  # Try different strategies for imputation\\n    'classifier__C': [0.1, 1, 10],  # Regularization strength for LogisticRegression\\n    'classifier__solver': ['liblinear', 'saga']  # Optimization algorithms\\n}\", '_i109': 'y_pred = model.predict(X_test)', '_i110': '# Accuracy\\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\\n\\n# Confusion Matrix\\nconf_matrix = confusion_matrix(y_test, y_pred)\\nsns.heatmap(conf_matrix, annot=True, fmt=\\'d\\')\\nplt.show()\\n\\n# Classification Report\\nprint(classification_report(y_test, y_pred))', '_i111': '# After fitting the model, you can use it to make predictions or assess its performance\\ny_pred = model.predict(X_test)', '_i112': '# After fitting the model, you can use it to make predictions or assess its performance\\ny_pred = model.predict(y_test)', '_i113': '# After fitting the model, you can use it to make predictions or assess its performance\\n# Correct way to make predictions\\ny_pred = model.predict(X_test)', '_i114': \"#Dependecies\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\", '_i115': \"# Assuming you've uploaded the CSV to your Google Colab session\\nheart_df = pd.read_csv('heart_disease_health_indicators.csv')\", '_i116': '#Identify missing values\\nmissing_values = heart_df.info()', '_i117': '#Removing the null values\\n\\ndf.isnull().sum()', '_117': HeartDiseaseorAttack    0\n",
            "HighBP                  0\n",
            "HighChol                0\n",
            "CholCheck               0\n",
            "BMI                     0\n",
            "Smoker                  0\n",
            "Stroke                  0\n",
            "Diabetes                0\n",
            "PhysActivity            0\n",
            "Fruits                  0\n",
            "Veggies                 0\n",
            "HvyAlcoholConsump       0\n",
            "AnyHealthcare           0\n",
            "NoDocbcCost             1\n",
            "GenHlth                 1\n",
            "MentHlth                1\n",
            "PhysHlth                1\n",
            "DiffWalk                1\n",
            "Sex                     1\n",
            "Age                     1\n",
            "Education               1\n",
            "Income                  1\n",
            "dtype: int64, '_i118': \"#Dependecies\\n\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\", '_i119': \"# Assuming you've uploaded the CSV to your Google Colab session\\nheart_df = pd.read_csv('heart_disease_health_indicators.csv')\", '_i120': '#Identify missing values\\nmissing_values = heart_df.info()', '_i121': '#Identify missing values\\n heart_df.info()', '_i122': '#Identify missing values\\nheart_df.info()', '_i123': '#Removing the null values\\n\\nheart_df.isnull().sum()', '_123': HeartDiseaseorAttack    0\n",
            "HighBP                  0\n",
            "HighChol                0\n",
            "CholCheck               0\n",
            "BMI                     0\n",
            "Smoker                  0\n",
            "Stroke                  0\n",
            "Diabetes                0\n",
            "PhysActivity            0\n",
            "Fruits                  0\n",
            "Veggies                 0\n",
            "HvyAlcoholConsump       0\n",
            "AnyHealthcare           0\n",
            "NoDocbcCost             0\n",
            "GenHlth                 0\n",
            "MentHlth                0\n",
            "PhysHlth                0\n",
            "DiffWalk                0\n",
            "Sex                     0\n",
            "Age                     0\n",
            "Education               0\n",
            "Income                  0\n",
            "dtype: int64, '_i124': \"columns_with_missing_values = ['Diabetes', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income']\\ndf_cleaned_specific = df.dropna(subset=columns_with_missing_values)\", '_i125': \"# Assuming there are no categorical variables to encode in this dataset\\n# Select features and target\\nX = df.drop('HeartDiseaseorAttack', axis=1)  # Features\\ny = df['HeartDiseaseorAttack']  # Target variable\", '_i126': 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)', '_i127': 'scaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)', '_i128': 'pip install scikit-learn', '_i129': 'from sklearn.linear_model import LogisticRegression', '_i130': 'model = LogisticRegression(random_state=42)', '_i131': 'from sklearn.ensemble import RandomForestClassifier\\n\\n# Example of initializing a RandomForestClassifier, which does use n_estimators\\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)', '_i132': 'print(y_train.head())', '_i133': 'print(y_train.isna().sum())', '_i134': '#If y_train contains missing values:\\ny_train = y_train.dropna()', '_i135': '!pip install scikit-learn\\nfrom sklearn.impute import SimpleImputer', '_i136': 'imputer = SimpleImputer(strategy=\"mean\")\\ny_train = imputer.fit_transform(y_train.values.reshape(-1, 1))', '_i137': \"from sklearn.impute import SimpleImputer\\nimport numpy as np\\n\\n# Create an imputer object with a strategy of filling missing values with the mean of the column\\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\\n\\n# Fit on the training data and transform it\\nX_train_imputed = imputer.fit_transform(X_train)\\n\\n# Don't forget to transform the test data as well, using the same imputer (without fitting it again)\\nX_test_imputed = imputer.transform(X_test)\\n\\n# Now you can fit the model with the imputed data\\nmodel.fit(X_train_imputed, y_train)\", '_137': RandomForestClassifier(random_state=42), '_i138': '# Drop rows with any missing values\\ndf_cleaned = df.dropna()\\n\\n# After cleaning, proceed with your feature/target selection, data splitting, and so on', '_i139': 'from sklearn.experimental import enable_hist_gradient_boosting  # noqa\\nfrom sklearn.ensemble import HistGradientBoostingClassifier\\n\\n# Initialize the model that can handle NaN values\\nmodel_with_nan_support = HistGradientBoostingClassifier()\\n\\n# This model can be directly fitted with data containing NaNs\\nmodel_with_nan_support.fit(X_train, y_train)', '_139': HistGradientBoostingClassifier(), '_i140': 'model.fit(X_train, y_train)', '_140': RandomForestClassifier(random_state=42), '_i141': '# After fitting the model, you can use it to make predictions or assess its performance\\n# Correct way to make predictions\\ny_pred = model.predict(X_test)', '_i142': \"# Define a pipeline\\npipeline = Pipeline([\\n    ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values\\n    ('scaler', StandardScaler()),  # Scale features\\n    ('classifier', LogisticRegression(random_state=42))  # Classifier\\n])\\n\\n# Define a grid of parameters to search\\nparam_grid = {\\n    'imputer__strategy': ['mean', 'median'],  # Try different strategies for imputation\\n    'classifier__C': [0.1, 1, 10],  # Regularization strength for LogisticRegression\\n    'classifier__solver': ['liblinear', 'saga']  # Optimization algorithms\\n}\", '_i143': 'y_pred = model.predict(X_test)', '_i144': '# Accuracy\\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\\n\\n# Confusion Matrix\\nconf_matrix = confusion_matrix(y_test, y_pred)\\nsns.heatmap(conf_matrix, annot=True, fmt=\\'d\\')\\nplt.show()\\n\\n# Classification Report\\nprint(classification_report(y_test, y_pred))', '_i145': \"# Define a pipeline\\npipeline = Pipeline([\\n    ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values\\n    ('scaler', StandardScaler()),  # Scale features\\n    ('classifier', LogisticRegression(random_state=42))  # Classifier\\n])\\n\\n# Define a grid of parameters to search\\nparam_grid = {\\n    'imputer__strategy': ['mean', 'median'],  # Try different strategies for imputation\\n    'classifier__C': [0.1, 1, 10],  # Regularization strength for LogisticRegression\\n    'classifier__solver': ['liblinear', 'saga']  # Optimization algorithms\\n}\\n\\ny_pred = model.predict(X_test)\", '_i146': \"# Define a pipeline\\npipeline = Pipeline([\\n    ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values\\n    ('scaler', StandardScaler()),  # Scale features\\n    ('classifier', LogisticRegression(random_state=42))  # Classifier\\n])\\n\\n# Define a grid of parameters to search\\nparam_grid = {\\n    'imputer__strategy': ['mean', 'median'],  # Try different strategies for imputation\\n    'classifier__C': [0.1, 1, 10],  # Regularization strength for LogisticRegression\\n    'classifier__solver': ['liblinear', 'saga']  # Optimization algorithms\\n}\", '_i147': '# Fit the pipeline on the training data\\npipeline.fit(X_train, y_train)\\n\\n# Predict on the test data using the pipeline\\ny_pred = pipeline.predict(X_test)', 'y_pred': array([0., 0., 0., ..., 0., 0., 0.]), '_i148': '# Accuracy\\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\\n\\n# Confusion Matrix\\nconf_matrix = confusion_matrix(y_test, y_pred)\\nsns.heatmap(conf_matrix, annot=True, fmt=\\'d\\')\\nplt.show()\\n\\n# Classification Report\\nprint(classification_report(y_test, y_pred))', 'conf_matrix': array([[24702,   283],\n",
            "       [ 2275,   313]]), '_i149': \"# Define a grid of parameters to search\\nparam_grid = {\\n    'imputer__strategy': ['mean', 'median'],  # Try different strategies for imputation\\n    'classifier__C': [0.1, 1, 10],  # Regularization strength for LogisticRegression\\n    'classifier__solver': ['liblinear', 'saga']  # Optimization algorithms\\n}\", '_i150': 'from sklearn.model_selection import GridSearchCV', '_i151': '# Define the pipeline and parameter grid\\npipeline = ...\\nparam_grid = ...', '_i152': \"# Instantiate the GridSearchCV object\\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose=1)\", '_i153': 'import pandas as pd', '_i154': 'print(X_train.head())\\nprint(y_train.head())', '_i155': 'X_train = pd.DataFrame(X_train)\\ny_train = pd.DataFrame(y_train)', '_i156': 'print(X_train.head())\\nprint(y_train.head())', '_i157': \"from google.colab import drive\\ndrive.mount('/content/drive')\", '_i158': '# Fit the GridSearchCV object\\ngrid_search.fit(X_train_scaled, y_train)', '_i159': 'from sklearn.model_selection import GridSearchCV', '_i160': 'X_train_scaled = # Load the scaled training data', '_i161': '# After fitting the model, you can use it to make predictions or assess its performance\\n# Correct way to make predictions\\ny_pred = model.predict(X_test)', '_i162': 'X_train_scaled = X_train_scaled(scaler.fit_transform(X_train))', '_i163': 'X_train_scaled = X_train_scaled(scaler.fit_transform(X_train_scaled))', '_i164': 'X_train_scaled = X_train_scaled(scaler.fit_transform(X_train_scaled))\\nX_train_scaled.df()', '_i165': 'print(X_train_scaled)', '_i166': 'X_train_scaled = X_train_scaled(scaler.fit_transform(X_train_scaled))\\nprint(X_train_scaled)', '_i167': 'print(X_train_scaled)', '_i168': 'X_train_scaled = X_train_scaled(scaler.fit_transform(X_train_scaled))\\nX_train_scaled.df()', '_i169': 'X_train= X_train(scaler.fit_transform(X_train))\\nX_train.df()', '_i170': 'print(X_train)', '_i171': 'X_train= X_train(scaler.fit_transform(X_train))\\nX_train.df()', '_i172': 'X_train= X_train(scaler.fit_transform(X_train))\\nX_train.head()', '_i173': 'X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=df.columns)\\nX_train_scaled.head()', '_i174': 'print(X_train.shape)\\nprint(df.columns.shape)', '_i175': 'num_cols_transformed = X_train_scaled.shape[1]\\nnum_cols_original = df.columns.shape[0]\\n\\nprint(f\"Number of columns in transformed data: {num_cols_transformed}\")\\nprint(f\"Number of columns in original DataFrame: {num_cols_original}\")', '_i176': 'num_cols_transformed = X_train.shape[1]\\nnum_cols_original = df.columns.shape[0]\\n\\nprint(f\"Number of columns in transformed data: {num_cols_transformed}\")\\nprint(f\"Number of columns in original DataFrame: {num_cols_original}\")', 'num_cols_transformed': 21, 'num_cols_original': 22, '_i177': 'X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=df.columns)\\nX_train_scaled.head()', '_i178': 'X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)', 'X_train_scaled':              0         1         2         3         4         5         6   \\\n",
            "0       1.15323 -0.861527  0.196519  0.223245 -0.896897 -0.207032 -0.425294   \n",
            "1       1.15323 -0.861527  0.196519  0.798741 -0.896897 -0.207032 -0.425294   \n",
            "2      -0.86713 -0.861527  0.196519 -0.783872 -0.896897 -0.207032 -0.425294   \n",
            "3      -0.86713 -0.861527  0.196519 -1.503241 -0.896897 -0.207032 -0.425294   \n",
            "4      -0.86713 -0.861527 -5.088560  0.510993  1.114955 -0.207032 -0.425294   \n",
            "...         ...       ...       ...       ...       ...       ...       ...   \n",
            "110285  1.15323  1.160730  0.196519  0.223245 -0.896897  4.830162  2.442677   \n",
            "110286  1.15323 -0.861527  0.196519  2.381353  1.114955 -0.207032 -0.425294   \n",
            "110287  1.15323  1.160730  0.196519  0.079371  1.114955 -0.207032 -0.425294   \n",
            "110288 -0.86713  1.160730  0.196519 -0.496124 -0.896897 -0.207032 -0.425294   \n",
            "110289  1.15323 -0.861527  0.196519  0.942614 -0.896897 -0.207032  2.442677   \n",
            "\n",
            "              7         8         9   ...        11        12        13  \\\n",
            "0      -1.780336  0.753199  0.484249  ...  0.225525 -0.295715  0.466729   \n",
            "1      -1.780336  0.753199  0.484249  ...  0.225525 -0.295715  0.466729   \n",
            "2      -1.780336  0.753199 -2.065052  ...  0.225525 -0.295715  1.404969   \n",
            "3       0.561692  0.753199  0.484249  ...  0.225525 -0.295715  0.466729   \n",
            "4       0.561692  0.753199  0.484249  ...  0.225525 -0.295715 -0.471510   \n",
            "...          ...       ...       ...  ...       ...       ...       ...   \n",
            "110285  0.561692  0.753199  0.484249  ...  0.225525 -0.295715  1.404969   \n",
            "110286  0.561692  0.753199 -2.065052  ...  0.225525  3.381634 -1.409750   \n",
            "110287  0.561692 -1.327670  0.484249  ...  0.225525 -0.295715  0.466729   \n",
            "110288  0.561692  0.753199  0.484249  ...  0.225525 -0.295715 -0.471510   \n",
            "110289 -1.780336  0.753199  0.484249  ...  0.225525 -0.295715  0.466729   \n",
            "\n",
            "              14        15        16        17        18       19        20  \n",
            "0      -0.425506 -0.483517 -0.445100 -0.884733  0.631794 -1.09985  0.928495  \n",
            "1      -0.425506  2.985784 -0.445100  1.130284 -0.351821 -0.07421  0.928495  \n",
            "2      -0.425506  1.829351  2.246685 -0.884733  0.959666 -1.09985  0.440835  \n",
            "3      -0.013376 -0.483517  2.246685 -0.884733 -0.023949  0.95143  0.440835  \n",
            "4      -0.150753 -0.483517 -0.445100 -0.884733 -1.663307 -0.07421  0.928495  \n",
            "...          ...       ...       ...       ...       ...      ...       ...  \n",
            "110285 -0.425506 -0.252230  2.246685 -0.884733  1.287537 -0.07421 -0.534485  \n",
            "110286  1.635143 -0.483517 -0.445100 -0.884733 -1.007564 -1.09985 -2.485125  \n",
            "110287 -0.425506 -0.483517 -0.445100  1.130284  0.959666  0.95143  0.440835  \n",
            "110288 -0.425506 -0.136587 -0.445100 -0.884733  0.303923  0.95143  0.928495  \n",
            "110289  1.635143 -0.483517  2.246685 -0.884733  0.959666 -0.07421 -0.046825  \n",
            "\n",
            "[110290 rows x 21 columns], '_i179': 'X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=df.columns)', '_i180': '# Fit the GridSearchCV object\\ngrid_search.fit(X_train_scaled, y_train)', '_i181': '# Fit the GridSearchCV object\\ngrid_search.fit(X_train, y_train)', '_i182': 'print(locals())'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(grid_search))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuP_30g-ZTf_",
        "outputId": "ceba1afe-d5f7-476b-9dd4-e3b35cead51d"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'sklearn.model_selection._search.GridSearchCV'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(grid_search))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFKOiydPZXKp",
        "outputId": "a87fcb34-0b04-4e07-bd7f-69a7ee1fc06d"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_check_feature_names', '_check_n_features', '_check_refit_for_multimetric', '_estimator_type', '_format_results', '_get_param_names', '_get_tags', '_more_tags', '_repr_html_', '_repr_html_inner', '_repr_mimebundle_', '_required_parameters', '_run_search', '_select_best_index', '_validate_data', '_validate_params', 'classes_', 'cv', 'decision_function', 'error_score', 'estimator', 'fit', 'get_params', 'inverse_transform', 'n_features_in_', 'n_jobs', 'param_grid', 'pre_dispatch', 'predict', 'predict_log_proba', 'predict_proba', 'refit', 'return_train_score', 'score', 'score_samples', 'scoring', 'set_params', 'transform', 'verbose']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Example setup (replace with your actual setup)\n",
        "param_grid = {'n_estimators': [100, 200], 'max_depth': [2, 5]}\n",
        "model = RandomForestClassifier()\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "\n",
        "# Fitting GridSearchCV\n",
        "grid_search.fit(X_train, y_train)  # Ensure this line executes successfully\n",
        "\n",
        "# Then access best_params_\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ll4lruaalVN",
        "outputId": "d515f919-982b-4765-bf42-ac1ed3dad021"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:686: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  estimator.fit(X_train, y_train, **fit_params)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:909: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  self.best_estimator_.fit(X, y, **fit_params)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'max_depth': 5, 'n_estimators': 100}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# More detailed performance analysis\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Axfpk8tgbqC4",
        "outputId": "7a8206e3-b2a0-4f87-be60-5569f583d7cb"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9072280854459073\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.99      0.95     24985\n",
            "           1       0.53      0.12      0.20      2588\n",
            "\n",
            "    accuracy                           0.91     27573\n",
            "   macro avg       0.72      0.55      0.57     27573\n",
            "weighted avg       0.88      0.91      0.88     27573\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "V4AxJdCnduGr"
      }
    }
  ]
}